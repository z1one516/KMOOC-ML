{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16cb5b3-dd53-48ca-a397-43936175dfa7",
   "metadata": {},
   "source": [
    "# CodeLab02 NLP \n",
    "Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f025207-06ca-422a-8abf-2f128c78716d",
   "metadata": {},
   "source": [
    "# Welcome to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7175941c-79ee-4e3c-86cd-9076d1a2a078",
   "metadata": {},
   "source": [
    "한국어로 \"자연어 처리\"라고 불리는 NLP는 인간의 언어를 컴퓨터가 묘사할 수 있도록 연구하는 인공지능의 주요 분야 중 하나입니다. \n",
    "\n",
    "즉, 기계가 인간의 언어를 알아들을 수 있도록 하는 일인 것입니다.\n",
    "\n",
    "NLP는 활용되는 분야가 다양한데, 문장의 빈칸을 예측하도록 모델을 구성할 수도 있고(prediction), 텍스트를 카테고리별로 분류시키도록 할 수도 있고(classification), 전체 문장을 번역하도록 할 수도 있습니다 (translation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdccb1-d04b-478c-b8e7-95dd3c1dfbc8",
   "metadata": {},
   "source": [
    "# NLP의 기본개념"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea8217a-f2eb-4cf9-a553-0691d6de0897",
   "metadata": {},
   "source": [
    "NLP의 기본개념에 대해 알아봅시다.  \n",
    "이 작업은 데이터 전처리에서 이루어지는 작업이며, 크게 총 7가지의 작업으로 나눌 수 있습니다.   \n",
    "그럼 지금부터 하나씩 예제를 살펴보며 학습해봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cc433-1410-4bac-bf86-89bbdc6dde9e",
   "metadata": {},
   "source": [
    "1. 토큰화\n",
    "1. 정제 및 정규화\n",
    "1. 표제어 추출\n",
    "1. 불용어 제거\n",
    "1. 정규 표현식\n",
    "1. 인코딩\n",
    "1. 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0914546-d887-4430-a4c8-89dd5e29d58f",
   "metadata": {},
   "source": [
    "## 토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a10781a-414d-4ee3-ab67-5342e6b3e66b",
   "metadata": {},
   "source": [
    "자연어 처리에서 데이터의 구성은 코퍼스(Corpus)들로 이루어져 있습니다. 코퍼스는 한국어로 말뭉치라는 뜻으로, 단어들의 집합입니다.\n",
    "\n",
    "코퍼스를 정해진 기준에 따라 문장, 단어, 또는 문자들로 나누는 작업을 토큰화라고 부릅니다.   \n",
    "따라서 기준을 어떻게 정하느냐에 따라 다양한 토큰화 방법이 나올 수 있습니다.   \n",
    "즉, 문장을 기준으로 토큰화를 진행하게 되면 문장 토큰화,   \n",
    "단어를 기준으로 토큰화를 진행하게 되면 단어 토큰화,   \n",
    "문자를 기준으로 트큰화를 진행하게 되면 문자 토큰화가 되는 것입니다.  \n",
    "토큰화의 종류에 대해 알아봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1550b-4b88-4607-9074-63edecec2090",
   "metadata": {},
   "source": [
    "### 단어 토큰화(Word Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b09c1b-6bbf-4ac9-8510-fe49c1f9e4da",
   "metadata": {},
   "source": [
    "`I like to learn AI.`라는 문장이 있다고 가정합시다.   \n",
    "구두점(punctuation)과 같은 문자(`` !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ ``)는 제외시키고,   \n",
    "단어로 토큰화를 진행하는 작업을 해보면, 다음과 같습니다. \n",
    "\n",
    "`출력: \"I\", \"like\", \"to\", \"learn\", \"AI\" `  \n",
    "\n",
    "이 예제는 단어 토큰화의 가장 기초적인 예제를 보여주는데,   \n",
    "그 이유는 구두점을 지운 후에 띄어쓰기(whitespace)를 기준으로만 토큰화를 진행했는데도 불구하고,   \n",
    "토큰화 작업이 잘 이루어졌기 때문입니다.\n",
    "\n",
    "하지만 보통 토큰화 작업은 단순히 구두점이나 특수문자만 제거하는 방법만으로는 한계가 있습니다.  \n",
    "왜냐하면 구두점이나 특수문자가 가지고 있는 의미가 사라질 수도 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0acfe2-692c-4df5-b2ae-2b0d88d15cdc",
   "metadata": {},
   "source": [
    "### 문장 토큰화(Sentence Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4575b84-3e6d-4ad1-8f09-04a9df08004b",
   "metadata": {},
   "source": [
    "이번에는 토큰의 단위가 문장일 경우에 진행하는 토큰화 방법인 문장 토큰화에 대해 살펴봅시다.     \n",
    "문장을 어떻게 나눌까요? 가장 쉬운 방법으로는 마침표를 기준으로 나눌 수 있겠죠.    \n",
    "하지만, 다양한 문장이 존재하기에 위의 것으로만 문장을 다 나누지 못할 경우도 충분히 발생할 수 있습니다.   \n",
    "그래서 우리는 다양한 상황에 토큰화가 성공적으로 이루어지도록 `규칙`을 잘 설정하는 것이 중요합니다.\n",
    "\n",
    "많은 개발자들은 성공적인 토큰화를 위해 그에 맞는 `패키지`를 제작하고 이를 오픈소스로 공유하고 있습니다.\n",
    "\n",
    "대표적으로 `NLTK`와 `KSS`가 있습니다.\n",
    "NLTK와 KSS는 install 하여 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebca70ab-bf59-4328-98de-5fe70c244084",
   "metadata": {},
   "source": [
    "`pip install nltk`\n",
    "\n",
    "`pip install kss`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1618b22d-4913-4f63-8bb9-e83e57958cd4",
   "metadata": {},
   "source": [
    "패키지를 import 하고 간단한 문장을 통해 문장토큰화를 진행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e9d138-0125-492f-be5f-c179d6f752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "import kss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d5f92-12ca-47ce-bc8f-1213f05e2d58",
   "metadata": {},
   "source": [
    "문장토큰화는 tokenize의 `sent_tokenize()`, kss는 `split_sentences()`를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358ea3c0-0daf-4265-b1af-f06375d3e9ca",
   "metadata": {},
   "source": [
    "__Expected Output:__\n",
    "\n",
    "['I got a chance to learn AI.', 'I will definitely study AI through this opportunity.', 'And NLP is really fun Moreover, I think I will be interested in AI.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ef875c-4cf1-4c4c-9c19-423faa3e85f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I got a chance to learn AI.', 'I will definitely study AI through this opportunity.', 'And NLP is really fun Moreover, I think I will be interested in AI.']\n"
     ]
    }
   ],
   "source": [
    "string = \"I got a chance to learn AI. I will definitely study AI through this opportunity. And NLP is really fun Moreover, I think I will be interested in AI.\"\n",
    "print(\"Your Code Here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a34b16-bf52-480a-8c41-c16ac4ed4f95",
   "metadata": {},
   "source": [
    "한국어로도 토큰화를 진행해봅시다.\n",
    "한국어는 kss 패키지를 사용합니다.\n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "['나는 AI를 배울 기회를 얻었어.', '이 기회를 통해 AI를 확실하게 공부할거야. 그리고 NLP는 정말 재밌는걸?', '더욱이 AI에 대해 흥미가 생길거 같아.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609bafea-0b11-4be1-82e2-4e80286aac2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는 AI를 배울 기회를 얻었어.', '이 기회를 통해 AI를 확실하게 공부할거야. 그리고 NLP는 정말 재밌는걸?', '더욱이 AI에 대해 흥미가 생길거 같아.']\n"
     ]
    }
   ],
   "source": [
    "text = '나는 AI를 배울 기회를 얻었어. 이 기회를 통해 AI를 확실하게 공부할거야. 그리고 NLP는 정말 재밌는걸? 더욱이 AI에 대해 흥미가 생길거 같아.'\n",
    "print(\"Your Code Here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfaefa4-0ded-49fc-bac3-a3d645de0265",
   "metadata": {},
   "source": [
    "## 정규 표현식(Regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88017432-6074-4739-8502-aefdd951393c",
   "metadata": {},
   "source": [
    "정규 표현식은 특정한 규칙을 가진 문자열의 집합을 표현하는 데 사용하는 형식 언어입니다.   \n",
    "정규표현식은 주로 문자열을 분리하거나, 필요없는 문자들만 제거할 때 주로 사용하는 문법입니다.\n",
    "\n",
    "파이썬에서는 정규 표현식을 위한 모듈 re를 지원하고 있습니다.   \n",
    "`import re`를 사용하여 re모듈을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ee513-2d57-451f-ba6d-d8e1d9750a9f",
   "metadata": {},
   "source": [
    "### 정규 표현식 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed6b49-3293-45f3-93e1-37c2cc368b13",
   "metadata": {},
   "source": [
    "| 문자 | 기능 |\n",
    "|---|---|\n",
    "| . | 임의의 문자 1개 |\n",
    "| ? | 문자 앞의 문자를 생략 가능 |\n",
    "| * | 0 <= 앞의 문자 <= 무한 |\n",
    "| + | 1 <= 앞의 문자 |\n",
    "| ^ | 문자열의 시작 |\n",
    "| $ | 문자열의 종료 |\n",
    "| {N} | N 만큼 반복 |\n",
    "| {N1, N2} | N1 <= X <= N2 만큼 반복 |\n",
    "| {N,} | N <= X 만큼 반복 |\n",
    "| [  ] | 대괄호 안의 문자들 중 한 개의 문자와 매치 |\n",
    "| [^문자] | 해당 문자를 제외한 문자 매치 |\n",
    "| l | OR |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab8163-3425-41e7-a7c8-f0c05a97db2a",
   "metadata": {},
   "source": [
    "### 정규 표현식 메소드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496e7e5-9d11-4d00-be9e-d13e84bcb738",
   "metadata": {},
   "source": [
    "| 메소드 | 기능 |\n",
    "|---|---|\n",
    "| re.compile() | 정규 표현식을 컴파일 한다. 찾고자 하는 패턴이 있을 때 컴파일 해놓고 찾아야 하므로, 가장 우선순위 함수이다. |\n",
    "| re.search() | 문자열 전체에 대해 정규표현식과 매치되는지 검색한다. |\n",
    "| re.match() | 문자열의 처음이 정규 표현식과 매치되는지 검색한다. |\n",
    "| re.split() | 정규 표현식을 기준으로 문자열을 분리하여 리스트 타입으로 반환한다. |\n",
    "| re.findall() | 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열을 리스트 타입으로 반환한다. |\n",
    "| re.finditer() | 문자열에서 정규 표현식과 매치되는 모든 경우의 문자열의 iterator를 반환한다. |\n",
    "| re.sub() | 문자열에서 정규 표현식과 일치하는 부분에 대해 다른 문자열로 대체한다. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353a02e9-1f58-42d9-bf2a-25c797c6d004",
   "metadata": {},
   "source": [
    "### 정규 표현식 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f37ad-e131-4c46-a388-901c31670c60",
   "metadata": {},
   "source": [
    "지금부터, 정규 표현식을 연습해봅시다.   \n",
    "먼저 정규 표현식을 사용하기 위해 `re` 모듈을 import하고 시작하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb988d6-ff7d-4c5b-9154-a87ddc7f3abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c964e-7122-432c-b6e2-7f584460ea2b",
   "metadata": {},
   "source": [
    "#### `.` 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4a916-cb2f-41d4-9a82-60315b404113",
   "metadata": {},
   "source": [
    "`.`은 한 개의 임의의 문자를 나타내는 문자입니다. 먼저 compile() 메소드를 사용하여 정규 표현식을 컴파일 합시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41d31909-b84c-4256-b466-ac21d075983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"a.t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7af95a-89bc-4af4-bd8f-e32a79177aa4",
   "metadata": {},
   "source": [
    "`.`에는 어떤 문자라도 올 수 있습니다. 따라서 해당 정규 표현식에 맞게 search()를 이용해서 act를 찾아보겠습니다.\n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "<re.Match object; span=(0, 3), match='act'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec09e34f-5417-4710-b722-d34f4b9788e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='act'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.search(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26250877-d102-400b-a201-adf0290774ed",
   "metadata": {},
   "source": [
    "위 예제는 `.`위치에 c를 넣었기 때문에 잘 찾은 것으로 보입니다. 그렇다면, a와 t의 범위를 넘어가는 알파벳을 찾게하면 어떻게 될까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12a06fc2-13a0-466b-9b81-1ad020d165c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='azt'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.search(\"azt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da49ad-367f-4061-8277-3be40b743686",
   "metadata": {},
   "source": [
    "잘 찾은 것을 볼 수 있네요.\n",
    "마찬가지로, `.`위치에는 어떤 문자도 올 수 있기 때문에 a와 t 사이의 범위는 문제되지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc1fd28-89bc-4458-a03e-74fc7e4bec7a",
   "metadata": {},
   "source": [
    "#### `?` 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47db55b5-b71f-40cc-aee7-26ddfcb8f348",
   "metadata": {},
   "source": [
    "`?`는 `?` 앞의 문자가 존재할 수도 있고, 존재하지 않을 수도 있는 문자입니다. ac?t라고 한다면, c는 자유로운 문자가 되어 이 경우에는 act, at 둘다 모두 매치할 수 있습니다.\n",
    "\n",
    "__Expected Output__:\n",
    "\n",
    "<re.Match object; span=(0, 3), match='act'>   \n",
    "<re.Match object; span=(0, 2), match='at'>   \n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44470ff3-7bd3-456c-b9b5-9d32cc830715",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"ac?t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e9710fe-1927-402b-bdbc-8ce86e9a42a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='act'>\n",
      "<re.Match object; span=(0, 2), match='at'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(None)\n",
    "print(None)\n",
    "print(p.search(\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91806b00-f280-4d81-ac24-b6ce1062f6f1",
   "metadata": {},
   "source": [
    "위 결과를 보면 매치될 수 있는 act와 at는 잘 매치가 된다고 검색되었지만, 경우의 수에 없는 a는 아무 결과도 출력되지 않는 것으로 보아 매치가 안 되는 것임을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869990c1-c51a-47bb-bd25-682c3f7e3604",
   "metadata": {},
   "source": [
    "#### `*` 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f12183-7c82-42df-a8fb-b6662bf77c83",
   "metadata": {},
   "source": [
    "`*`은 앞의 문자가 무한개로 존재할 수 있는 경우를 나타냅니다. 0개도 포함됩니다. a*ct라면 ct, act, aact, aaact 등 a의 개수는 무한대가 될 수 있습니다.\n",
    "\n",
    "`+`문자도 이와 비슷하나, 앞의 문자가 최소 1개부터 시작된다는 점이 다릅니다. 나머지는 같습니다.\n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "<re.Match object; span=(0, 2), match='ct'>   \n",
    "<re.Match object; span=(0, 6), match='aaaact'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a9393b-3782-407b-bcd8-dc65ca7d6d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"a*ct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40cad90a-c38b-4b22-9d16-127829ee7e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ct'>\n",
      "<re.Match object; span=(0, 6), match='aaaact'>\n"
     ]
    }
   ],
   "source": [
    "print(None)\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4646d964-82ea-49df-b9a5-fa0f4fb6d023",
   "metadata": {},
   "source": [
    "#### `^` 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e722bb53-58d5-4fd4-93d3-89e8ba208895",
   "metadata": {},
   "source": [
    "`^`은 시작되는 문자열을 지정합니다. ^act라면 문자열 act로 시작되는 경우들만 매치합니다.\n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "<re.Match object; span=(0, 3), match='act'>   \n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e109d2dc-c624-4d86-8785-4104db87c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"^act\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19d920c7-1806-46d5-a9e1-e84c519532dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='act'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(None)\n",
    "print(p.search(\"ac\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7d686-0295-48ee-990b-c72df5b92324",
   "metadata": {},
   "source": [
    "위의 결과 act로 시작하는 acts는 잘 매치된 것을 알 수 있지만, ac는 그렇지 않음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5e080d-e8cb-49dd-bfd9-f502eb8a4bd9",
   "metadata": {},
   "source": [
    "그런데 `^`은 `[]`와 함께 사용할 시, `부정`의 의미를 나타냅니다.   \n",
    "예를들어, \"%[^a-zA-Z]%\"는 두 퍼센트 기호 사이의 문자가 \"아닌\" 문자열과 일치한다는 의미입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd303c82-932e-4c72-a5e0-5d89821033b6",
   "metadata": {},
   "source": [
    "#### `{숫자}` 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99454bdf-3eeb-4b2d-80a7-b27ae09327ff",
   "metadata": {},
   "source": [
    "바로 앞의 문자를 해당 숫자만큼 반복한 것을 나타내는 문자입니다. ac{3}t라면 a와 t 사이에 c가 존재하면서 c의 개수가 총 3개일때만 매치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb265ff8-e480-48bc-8c7f-560bff41bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"ac{3}t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2409b9f-2386-405e-8f0c-8a3ece993d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 5), match='accct'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(p.search(\"act\"))\n",
    "print(p.search(\"acct\"))\n",
    "print(p.search(\"accct\"))\n",
    "print(p.search(\"acccct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e6f27-6bd6-49a4-b37a-1fc2082de37e",
   "metadata": {},
   "source": [
    "`{숫자1, 숫자2}`나 `{숫자,}` 문자도 같은 원리입니다. `{숫자1, 숫자2}`는 바로 앞의 문자의 개수가 숫자1 이상, 숫자2 이하 만큼 반복한 것을 나타내는 것입니다.\n",
    "\n",
    "`{숫자,}`는 앞의 문자의 개수가 숫자 이상 만큼 반복한 것을 나타내는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbb36664-0748-42ff-b26d-bb93bc5356fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = re.compile(\"ac{1,3}t\")\n",
    "p2 = re.compile(\"ac{2,}t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51f60405-294f-4835-b9d2-9574994a8b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='act'>\n",
      "<re.Match object; span=(0, 5), match='accct'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(p1.search(\"act\"))\n",
    "print(p1.search(\"accct\"))\n",
    "print(p1.search(\"acccct\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a34a14-8660-4ed1-a171-24f66ac1e2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 4), match='acct'>\n",
      "<re.Match object; span=(0, 5), match='accct'>\n"
     ]
    }
   ],
   "source": [
    "print(p2.search(\"act\"))\n",
    "print(p2.search(\"acct\"))\n",
    "print(p2.search(\"accct\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07eb6f3-fbbf-46e8-8b82-7c13b9369dbf",
   "metadata": {},
   "source": [
    "#### `[]` 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a0639e-1fc7-41bc-ad6f-9f7931401577",
   "metadata": {},
   "source": [
    "`[]`는 안에 문자들을 넣으면 그 문자들 중에서 하나라도 매치되는것을 나타내는 문자입니다.\n",
    "\n",
    "[act]라면, a, c, t가 들어가있는 문자열과 매치됩니다. [a-z]처럼 a부터 z까지 범위를 지정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08ccedc6-877c-4411-92b6-3418fb8c33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"[act]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a9c5308-3f62-4c48-8804-873d13c76aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='c'>\n",
      "None\n",
      "<re.Match object; span=(0, 1), match='t'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(p.search(\"c\"))\n",
    "print(p.search(\"d\"))\n",
    "print(p.search(\"tttt\"))\n",
    "print(p.search(\"a-z\"))\n",
    "print(p.search(\"u-z\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d67fe9-4061-46e3-978a-9dfcc21c1343",
   "metadata": {},
   "source": [
    "a부터 z까지 범위를 지정하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5b1e778-612c-4257-9fef-3850d4e4fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3098c00-8bca-4164-9d56-2a1932c0e6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='c'>\n",
      "<re.Match object; span=(0, 1), match='d'>\n",
      "<re.Match object; span=(0, 1), match='t'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='u'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(p.search(\"c\"))\n",
    "print(p.search(\"d\"))\n",
    "print(p.search(\"tttt\"))\n",
    "print(p.search(\"a-z\"))\n",
    "print(p.search(\"u-z\"))\n",
    "print(p.search(\"A-Z\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bab7a-14aa-4486-b710-24df6960d3b6",
   "metadata": {},
   "source": [
    "__Expected Output:__\n",
    "\n",
    "<re.Match object; span=(0, 1), match='c'>  \n",
    "<re.Match object; span=(0, 1), match='d'>  \n",
    "<re.Match object; span=(0, 1), match='t'>  \n",
    "<re.Match object; span=(0, 1), match='a'>  \n",
    "<re.Match object; span=(0, 1), match='u'>  \n",
    "<re.Match object; span=(0, 1), match='A'>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13b692ad-7850-42ed-9d48-6a71a54d72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"[a-zA-Z]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a2b6d74-519c-4877-ac91-205a844b648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='c'>\n",
      "<re.Match object; span=(0, 1), match='d'>\n",
      "<re.Match object; span=(0, 1), match='t'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 1), match='u'>\n",
      "<re.Match object; span=(0, 1), match='A'>\n"
     ]
    }
   ],
   "source": [
    "print(None)\n",
    "print(None)\n",
    "print(None)\n",
    "print(None)\n",
    "print(None)\n",
    "print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d7cde-a8dc-418d-8ad6-bde95b0aac83",
   "metadata": {},
   "source": [
    "#### `[^문자]` 문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75682d26-6a52-4ab5-a78c-d034d719bf41",
   "metadata": {},
   "source": [
    "`[^문자]`는 ^뒤에 있는 모든 문자들을 제외시키는 역할을 합니다. 즉, ^뒤에 있는 모든 문자가 들어간 문자열이라면 제외시키고 매치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9d62244-ab89-4bd3-97c9-30d1a718e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(\"[^act]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00d0e4d3-ee41-4874-9023-88d1607c1e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 1), match='b'>\n"
     ]
    }
   ],
   "source": [
    "print(p.search(\"a\"))\n",
    "print(p.search(\"ac\"))\n",
    "print(p.search(\"b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885399f6-248a-4e79-b99f-04f81809110d",
   "metadata": {},
   "source": [
    "a부터 u까지 문자를 제외시키는 코드를 작성해보세요.\n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "<re.Match object; span=(4, 5), match='y'>  \n",
    "<re.Match object; span=(2, 3), match='v'>  \n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a0628ab-6ed6-429a-948e-f84385fad01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aac55ad3-35a5-4a8c-9c2d-d205c067cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 5), match='y'>\n",
      "<re.Match object; span=(2, 3), match='v'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(p.search(\"happy\"))\n",
    "print(p.search(\"love\"))\n",
    "print(p.search(\"nlp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671f934-cdf0-48de-8176-e02659ccf7c6",
   "metadata": {},
   "source": [
    "#### `\\`문자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cafcb5-308d-4b6d-ad2e-1db8b280f541",
   "metadata": {},
   "source": [
    "`\\ ^ $ . | [ ] ( ) * + ? { }` 이러한 문자들은 정규 표현식에서 특수한 문자입니다.   \n",
    "이러한 특수한 문자들을 추출하고 싶을 때에는 `\\`를 붙여 escape해주면 됩니다.   \n",
    "여러분이 익숙한 줄바꿈 문자 \\n, 탭 문자 \\t 등의 기능들도 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f2759-ac33-461a-a070-171fded7aba0",
   "metadata": {},
   "source": [
    "#### 기본적인 정규식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a7efe-1e4f-46f7-a0dd-b199e9527efd",
   "metadata": {},
   "source": [
    "가장 많이 쓰이는 기본적인 정규식은 다음과 같습니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe51b97-5464-41d7-9423-f2853274110a",
   "metadata": {},
   "source": [
    "- `^[0-9]*$`: 숫자   \n",
    "- `^[a-zA-Z]*$`: 영문자   \n",
    "- `^[가-힣]*$`: 현대 한글(유니코드를 지원하는 정규식 엔진에 한정)   \n",
    "- `^[ㄱ-ㅎㅏ-ㅣ가-힣]*$`: 한글 자모 낱자를 포함한 모든 현대 한글   \n",
    "- `^[a-zA-Z0-9]*$`: 영문자와 숫자 모두"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7fa485-be1f-496c-9cab-370d25170dd6",
   "metadata": {},
   "source": [
    "## 표제어 추출(Lemmatization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a00b61e-0872-425f-979f-aa5264757096",
   "metadata": {},
   "source": [
    "다음으로, 표제어 추출에 대해 살펴봅시다.  \n",
    "`표제어 추출`은 품사를 보존하며, 비슷한 단어의 뿌리 단어를 찾는 것입니다.  \n",
    "또한, `어간 추출`은 표제어 추출과 비슷하나, 어간 추출은 표제어 추출과 달리 품사가 보존되지 않는다는 특징이 있습니다.   \n",
    "표제어 추출과 어간 추출의 목적은 하나의 단어로 통합시킬 수 있다면, 통합시켜 단어 수를 줄이는 것입니다.\n",
    "\n",
    "- 표제어 추출의 예를 들어보겠습니다.   \n",
    "예를들어, \"is, are, am\"은 다른 단어이지만 그 뿌리 단어는 \"be\"로 동일합니다.  \n",
    "그래서 만약 \"are\"를 표제어 추출을 한다면 \"be\"가 나올 것입니다.\n",
    "\n",
    "- 어간 추출은 말 그대로 어간을 추출하는 작업입니다.  \n",
    "형태소는 어간(stem)과 접사(affix)로 구성되어있는데, 단어의 핵심 의미를 담고 있는 부분이 어간이고 추가적인 의미를 덧붙이는 부분이 접사입니다.   \n",
    "그래서 어간 추출은 단어에서 어간만 추출하는 것이라고 생각하면 됩니다.   \n",
    "예를들어, \"정확한\"이라는 뜻을 가진 \"accurate\"를 어간 추출을 진행할 경우 어간인 \"accur\"만 추출됩니다.   \n",
    "\"accur\"를 통해 품사가 보존되지 않는다는 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54725a1b-3b8a-4288-a94c-bc92ee125fcb",
   "metadata": {},
   "source": [
    "먼저, nltk 패키지를 import 해주도록 하겠습니다.   \n",
    "그리고 download를 진행해줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d23f3b65-c1fb-4799-9a9d-e80815200196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bb929c2-eb32-422b-8618-56467f897640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf040e-cc14-4b10-afda-8f1a539c6cc0",
   "metadata": {},
   "source": [
    "이제 nltk에서 표제어 추출을 진행해보겠습니다.   \n",
    "먼저 WordNetLemmatizer()를 import 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08b6affe-461f-456b-84a8-ad67c79d5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1aa555-d86c-432f-b64b-1a4ca64d2d01",
   "metadata": {},
   "source": [
    "그리고, `lem`이라는 객체를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aeae12db-7caf-45e4-83da-62412cd2d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89a786-cf39-4a5e-a154-dcf4f7653c35",
   "metadata": {},
   "source": [
    "표제어 추출을 진행할 단어 리스트를 만들어줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e04b8208-2551-4439-b59e-aeb2f4c8c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['\\n', 'flies', 'the', 'are', 'lives', 'nlp', 'happy', 'leaves']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f83e8-3e6b-4e06-96db-564bdcb0b6be",
   "metadata": {},
   "source": [
    "이제, ListComprehension을 사용하여 표제어 추출을 진행해봅시다.   \n",
    "표제어 추출은 `lemmatize()` 메소드를 사용하여 진행하도록 하겠습니다.\n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "['\\n', 'fly', 'the', 'are', 'life', 'nlp', 'happy', 'leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c414b58b-fd19-4d33-ac73-8767b1e80bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'fly', 'the', 'are', 'life', 'nlp', 'happy', 'leaf']\n"
     ]
    }
   ],
   "source": [
    "print(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a9072-98dc-46c2-85e0-1606ee786da8",
   "metadata": {},
   "source": [
    "## 불용어 제거(Remove stop words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c398f653-412e-45b2-935e-0d2e9cbc63bd",
   "metadata": {},
   "source": [
    "불용어란 \"큰 의미가 없는 단어 토큰\"을 의미하고, 이러한 불용어를 제거해주는 작업을 하려고 합니다.   \n",
    "불용어를 제거함으로써 데이터의 품질을 높여줄 수 있습니다.  \n",
    "만약, 불용어를 제거하지 않는다면 모델이 큰 의미가 없는 단어도 다른 단어들과 마찬가지로 학습하기 때문에   \n",
    "불필요한 학습과 더불어 모델의 성능을 저해할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c82de6-ea9c-4f5a-94d9-d95d0f48a2eb",
   "metadata": {},
   "source": [
    "불용어 제거는 nltk의 `stopwords` 패키지를 import하여 사용할 수 있습니다.   \n",
    "영어 불용어를 상위 10개만 확인해봅시다. 슬라이싱을 사용하여 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b84bfd4e-6a4b-4c5d-9867-25b405aea02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english')[:10])\n",
    "print(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ad1ee-95a5-4898-9b10-72f966d9981d",
   "metadata": {},
   "source": [
    "stop_words를 만들고 List Comprehension을 사용하여 text에서 불용어와 불용어가 아닌 것들을 분리해주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f799813d-cb53-4746-a467-e1627f131fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  \n",
    "\n",
    "stop_words = set(stopwords.words('english'))  \n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f6a896-1b93-466d-975e-c42cc1eff6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['\\n', 'flies', 'the', 'are', 'lives', 'nlp', 'happy', 'leaves']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ad3be",
   "metadata": {},
   "source": [
    "__Expected Output:__\n",
    "\n",
    "No Stop Words:  ['\\n', 'flies', 'lives', 'nlp', 'happy', 'leaves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a59f8c2-6393-4945-b8bf-d68fb3280102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Stop Words:  ['\\n', 'flies', 'lives', 'nlp', 'happy', 'leaves']\n"
     ]
    }
   ],
   "source": [
    "no_sw = [None for None in None if not None in None]\n",
    "print(\"No Stop Words: \", no_sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f9b57",
   "metadata": {},
   "source": [
    "__Expected Output:__\n",
    "\n",
    "Stop Words:  ['the', 'are']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b77f848-0ecb-4b9c-88a7-f40a53893486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words:  ['the', 'are']\n"
     ]
    }
   ],
   "source": [
    "sw = [None for None in None if None in None]\n",
    "print(\"Stop Words: \", sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da659ee8-8ba7-4056-a677-8881e1e6b153",
   "metadata": {},
   "source": [
    "## 인코딩 & 워드 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2db50b-4179-4479-961c-e8ff4bd8a99d",
   "metadata": {},
   "source": [
    "인코딩은 단어를 컴퓨터가 알아 들을 수 있도록 바꾸어주는 작업을 의미하며, 대표적인 예로 원 핫 인코딩이 있습니다.\n",
    "\n",
    "원 핫 인코딩은 해당 단어의 인덱스의 값만 1이고, 나머지는 다 0으로 표현되는 벡터 표현 방식입니다.   \n",
    "원 핫 인코딩은 준수한 성능을 나타내지만, 코퍼스 단어가 늘어날수록 벡터의 공간적 낭비라는 단점때문에 다른 인코딩 방식이 필요합니다.   \n",
    "\n",
    "그래서 나온 개념이 워드 임베딩입니다.   \n",
    "워드 임베딩은 단어를 밀집 벡터(dense vector)의 형태로 표현하는 방법입니다.   \n",
    "원 핫 인코딩의 경우에는 코퍼스가 50,000개라면 벡터의 차원도 50,000이 되기 때문에 부담이 있었습니다. 하지만, 워드 임베딩은 지정된 값으로 차원이 설정되며 안의 값은 0과 1이 아닌 해당 단어를 표현하기 위한 적절한 값으로 표현됩니다.   \n",
    "\n",
    "대표적인 워드 임베딩 방식론으로는 Word2Vec, Glove, FastText 등이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f82ad9-93c8-4374-8799-a1c9dc8f0cba",
   "metadata": {},
   "source": [
    "워드 임베딩은 tensorflow와 pytorch에서 모듈로 제공하고 있습니다.   \n",
    "Embedding()의 첫번째 파라미터는 input 차원을 의미하고, 두번째 파라미터는 output 차원을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c617d55b-6aef-4600-bfd6-81929cbf4290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x1f4baa36910>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "input_dim = 8\n",
    "output_dim = 4\n",
    "\n",
    "Embedding(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396bcd0-2d8e-4eae-8e8b-03c599159fd1",
   "metadata": {},
   "source": [
    "또한, 이는 행렬형태의 데이터로 수치화 되어 저장되기 때문에 이를 살펴보면 아래와 같습니다.   \n",
    "저희는 input_dim으로 8, output_dim으로 4를 주었기 때문에 (8, 4)의 행렬이 나온 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ed1c499-8e42-4542-b918-7d9ca62b5769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2925, -0.3290,  0.2315,  0.8325],\n",
       "        [-0.5571, -0.1664, -0.1167, -1.4422],\n",
       "        [ 0.9532,  1.2361,  0.8348, -0.4167],\n",
       "        [-0.7913, -1.0215, -1.1965, -1.6036],\n",
       "        [-0.2554, -0.2007,  0.1647,  0.4585],\n",
       "        [ 0.5649,  0.5459, -0.1485, -0.7012],\n",
       "        [-0.4538,  0.5273, -0.0115,  1.2572],\n",
       "        [-0.7760,  0.1099,  0.0096, -0.4104]], requires_grad=True)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "nn.Embedding(input_dim, output_dim).weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb72ca9-26fd-438f-91e5-3d56bfe4178d",
   "metadata": {},
   "source": [
    "## 패딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a61663-c263-435d-849a-3b1cd8a4bc42",
   "metadata": {},
   "source": [
    "우리가 사용하는 단어 데이터는 아쉽게도 길이가 다 재각각 입니다.   \n",
    "즉, 데이터셋의 모든 단어의 길이가 서로 일치하지 않을 경우가 대부분입니다. \n",
    "\n",
    "그러기 때문에 미니배치를 만들어 줄 때,   \n",
    "배치 크기를 기준으로, 단어의 길이가 끝나도 단어 뒤에 패딩을 넣어줍니다.  \n",
    "그렇게 되면 모든 데이터는 같은 크기를 가지게 됩니다.\n",
    "\n",
    "또한, 주로 패딩은 0값을 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02a26a-8ed4-4001-acb0-86a2059612cb",
   "metadata": {},
   "source": [
    "우선, keras의 Tokenizer를 사용하여 tokenizer 객체를 만들고, text를 토큰화 해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff98498d-7d07-47fd-a404-24dc663f639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a12311b5-ae43-4774-8170-658b763bf8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a56db2a-9226-4509-b855-6a8729a892f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['봄 여름 가을 겨울', '남자 여자', '초등 중등 고등']\n"
     ]
    }
   ],
   "source": [
    "text = ['봄 여름 가을 겨울', '남자 여자', '초등 중등 고등']\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb86acc-2a7a-4f91-9ef4-e7caf5bcb609",
   "metadata": {},
   "source": [
    "tokenizer의 `fit_on_texts()`를 이용하여 문자열을 단어 집합으로 생성해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fc3adf6-b614-4840-a97b-e87de3c512bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357f7c85-994e-4868-b2d5-af5077f6e149",
   "metadata": {},
   "source": [
    "그리고 이제 text를 인코딩해주도록 하겠습니다.   \n",
    "tokenizer의 `texts_to_sequences()`를 사용하여 인코딩을 진행할 수 있습니다.\n",
    "\n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "[[2, 8, 9, 10], [11, 12], [13, 14, 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33aced47-1643-44b7-af3a-1b14c3656e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 8, 9, 10], [11, 12], [13, 14, 15]]\n"
     ]
    }
   ],
   "source": [
    "encoded = None\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67062c95-339a-4f89-a564-8199e888ffa6",
   "metadata": {},
   "source": [
    "위의 예시를 보면, 모든 단어가 고유한 정수 값으로 변환되었습니다.  \n",
    "그런데, 전체 데이터 집합의 크기가 제각각입니다.  \n",
    "이를 맞추어주기 위해 패딩을 넣도록 하겠습니다.   \n",
    "\n",
    "패딩을 이용하여 모두 동일한 길이로 맞추어 주겠습니다.   \n",
    "여기서는 max_len을 8로 하겠습니다.\n",
    "\n",
    "인자로 padding='post'을 주면 패딩이 뒤에 붙도록 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd7c48ef-8a42-471b-b156-5bb3a80e751f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b87a3",
   "metadata": {},
   "source": [
    "__Expected Output:__\n",
    "\n",
    "```python\n",
    "array([[ 2, 17, 18, 19,  0,  0,  0,  0],\n",
    "       [20, 21,  0,  0,  0,  0,  0,  0],\n",
    "       [22, 23, 24,  0,  0,  0,  0,  0]])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e849e10e-16ea-4ec0-b851-931f277fcf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 17, 18, 19,  0,  0,  0,  0],\n",
       "       [20, 21,  0,  0,  0,  0,  0,  0],\n",
       "       [22, 23, 24,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_sequence(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15b875f-b0b4-48b7-83ff-11fc75c6fd5d",
   "metadata": {},
   "source": [
    "이렇게 하면, 데이터가 일정한 크기로 배열이 되었기 때문에 `행렬 연산이 가능하게 됩니다.`  \n",
    "행렬 연산이 가능하게 되었으므로, 인공지능 모델의 Layer끼리 연산을 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef0afc-1d3f-440d-a152-50255a00ec54",
   "metadata": {},
   "source": [
    "# 영화 리뷰 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90cccc-aadb-4e0f-8738-0ace305d5ecc",
   "metadata": {},
   "source": [
    "이제, NLP 데이터 전처리에 대한 큰 구조는 다 끝났습니다.   \n",
    "다음으로는 영화 리뷰 감성 분석이라는 주제를 가지고 NLP를 실습해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e89131-c83f-45f2-a6fe-a5c26b21f638",
   "metadata": {},
   "source": [
    "## 데이터셋 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827610fb-42c2-4f5b-b0e1-93a5e1f02a33",
   "metadata": {},
   "source": [
    "keras에서 제공하는 IMDB 데이터셋을 이용하여 영화 리뷰 감정 분석을 해보겠습니다.   \n",
    "우선, 필요한 모듈을 import하고 데이터셋을 불러오도록 하겠습니다.  \n",
    "데이터는 총 50,000개의 데이터로 구성되어 있습니다.  \n",
    "load_data()의 인자로 num_words를 사용하게 되면, 단어 집합의 크기가 값에 해당하는 숫자만큼 설정됩니다.  \n",
    "여기서는 num_words=5000을 인자로 주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1feeea3f-afe3-48ac-9613-85e38ef51282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e4de92d-10f9-44c4-ad00-cc74837f994d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\lkjs8\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lkjs8\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = None\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4d769-5ddc-45ba-9bc3-23fd86950765",
   "metadata": {},
   "source": [
    "결과를 보니 25,000개의 train data와 25,000개의 validation data가 잘 들어온 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1747871-166e-4c39-b226-1ba872ebe796",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563a5e3-9d1a-4882-a9ee-78c875caccf0",
   "metadata": {},
   "source": [
    "이제 좀더 세부적으로 train data와 validation data가 어떻게 구성되어 있는지,   \n",
    "각 데이터의 레이블은 어떻게 구성되어 있는지 한번 확인해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9989daa-de15-471c-8401-cbb5b1ca5476",
   "metadata": {},
   "source": [
    "먼저 데이터의 첫번째 값을 print하여 데이터를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "835f6d58-8919-4137-9299-1cf3d4d52147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "y_train:  1\n",
      "\n",
      "x_val:  [1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 1035, 1035, 1035, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 1668, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]\n",
      "y_val:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train: \", x_train[0])\n",
    "print(\"y_train: \", y_train[0], end='\\n\\n')\n",
    "\n",
    "print(\"x_val: \", x_val[0])\n",
    "print(\"y_val: \", y_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec8ded-fc62-425d-b021-4a5c95395e37",
   "metadata": {},
   "source": [
    "이처럼, IMDB 데이터셋은 리뷰의 단어들이 각각 인코딩 되어 있으며,   \n",
    "하나의 리뷰에 대해 긍정 레이블인 1과 부정 레이블인 0으로 매핑되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb53e2e-a894-4f72-bc20-4cc1e66c3bbb",
   "metadata": {},
   "source": [
    "그렇다면, 인코딩되기 전의 단어는 어떻게 확인할 수 있을까요?   \n",
    "바로, IMDB의 `get_word_index().items()`를 사용하면 됩니다.\n",
    "\n",
    "`get_word_index().items()`을 사용하여 인코딩 된 데이터를 디코딩 하여 원래 데이터를 찾아봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f03c73b9-0c2f-4c53-8f0b-f0a222c3a0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict_items'>\n"
     ]
    }
   ],
   "source": [
    "word2idx = imdb.get_word_index()\n",
    "items = word2idx.items()\n",
    "print(type(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45c5fbc-0c98-4184-bdf4-9e6e66246b3e",
   "metadata": {},
   "source": [
    "데이터 타입을 살펴보면 items는 key랑 value로 이루어진 dictionary 타입임을 알 수 있습니다.  \n",
    "그러면 딕셔너리의 key랑 value를 확인해봅시다.   \n",
    "\n",
    "__Expected Output:__\n",
    "\n",
    "```\n",
    "key:  fawn\n",
    "value:  34701\n",
    "key:  tsukino\n",
    "value:  52006\n",
    "key:  nunnery\n",
    "value:  52007\n",
    "key:  sonja\n",
    "value:  16816\n",
    "key:  vani\n",
    "value:  63951\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569069e2-4fc2-424b-afa3-37118cc28c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key:  fawn\n",
      "value:  34701\n",
      "key:  tsukino\n",
      "value:  52006\n",
      "key:  nunnery\n",
      "value:  52007\n",
      "key:  sonja\n",
      "value:  16816\n",
      "key:  vani\n",
      "value:  63951\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "for None, None in None:\n",
    "    if idx >= 5: break\n",
    "    print(\"key: \", key)\n",
    "    print(\"value: \", value)\n",
    "    idx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5c23f-807f-4957-a1ce-80864edb0e5d",
   "metadata": {},
   "source": [
    "이제 index와 단어를 매핑하는 idx2word를 만들어 보겠습니다.   \n",
    "패딩을 의미하는 $<pad>$는 0, 문장의 시작을 의미하는 $<sos>$는 1, 알수없음을 의미하는 $<unk>$는 2로 한다는 IMDB의 규칙을 따르겠습니다.   \n",
    "또한, 인덱스 3은 사용하지 않는다는 규칙에 의해 저희는 이들을 제외하고 index + 3을 하여 단어와 매핑시켜 dictionary를 만들겠습니다.\n",
    "\n",
    "먼저, 특수토큰들을 dictionary에 넣어주고 나머지 단어들도 넣어주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69eb7e21-4c63-487f-8402-ff33e7ee28f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {}\n",
    "special_tokens = [\"<pad>\", \"<sos>\", \"<unk>\"]\n",
    "\n",
    "for idx, token in enumerate(special_tokens):\n",
    "    idx2word[idx] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a29d7c7-112f-453e-aabe-7e76b7ee52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in items:\n",
    "    idx2word[value+3] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d280274-ad0a-4a04-a0ef-734ca6765d51",
   "metadata": {},
   "source": [
    "또한, IDMB에서는 데이터셋에서 빈도수가 가장 높은 단어순으로 인덱싱이 되어 있습니다.   \n",
    "따라서 빈도수가 가장 높은 단어는 index=4인 단어입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee2fcee-0aca-4c31-9753-6095cf33f5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도수 가장 높은 단어 : the\n"
     ]
    }
   ],
   "source": [
    "print('빈도수 가장 높은 단어 : {}'.format(idx2word[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b25457-9d56-41ae-8be9-f5259b620ce5",
   "metadata": {},
   "source": [
    "다음으로는 패딩을 넣어주도록 하겠습니다. 데이터를 살펴보죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "176c7741-7b43-453a-8644-0077fc25c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train:  [list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train: \", x_train[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053d559-2e0e-4e1b-9e42-7621bb580ed7",
   "metadata": {},
   "source": [
    "데이터를 살펴보면 그 길이가 서로 일치하지 않는 것을 알 수 있습니다.   \n",
    "데이터의 길이를 일치시켜 모델이 학습에 도움을 줄 수 있도록 해줄 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276b2ce9-0632-4f43-8e2d-ef88ee3cffa2",
   "metadata": {},
   "source": [
    "패딩은 keras의 pad_sequences()를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "168b5c74-8b89-4712-a8a0-ae982922411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 200\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen, padding='post')\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e67e7f44-3745-4a63-b1c7-43f3f08ee4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   5,   25,  100,   43,  838,  112,   50,  670,    2,    9,   35,\n",
       "         480,  284,    5,  150,    4,  172,  112,  167,    2,  336,  385,\n",
       "          39,    4,  172, 4536, 1111,   17,  546,   38,   13,  447,    4,\n",
       "         192,   50,   16,    6,  147, 2025,   19,   14,   22,    4, 1920,\n",
       "        4613,  469,    4,   22,   71,   87,   12,   16,   43,  530,   38,\n",
       "          76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,   16,\n",
       "         626,   18,    2,    5,   62,  386,   12,    8,  316,    8,  106,\n",
       "           5,    4, 2223,    2,   16,  480,   66, 3785,   33,    4,  130,\n",
       "          12,   16,   38,  619,    5,   25,  124,   51,   36,  135,   48,\n",
       "          25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,    5,\n",
       "          14,  407,   16,   82,    2,    8,    4,  107,  117,    2,   15,\n",
       "         256,    4,    2,    7, 3766,    5,  723,   36,   71,   43,  530,\n",
       "         476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,  104,\n",
       "          88,    4,  381,   15,  297,   98,   32, 2071,   56,   26,  141,\n",
       "           6,  194,    2,   18,    4,  226,   22,   21,  134,  476,   26,\n",
       "         480,    5,  144,   30,    2,   18,   51,   36,   28,  224,   92,\n",
       "          25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,   16,\n",
       "         283,    5,   16, 4472,  113,  103,   32,   15,   16,    2,   19,\n",
       "         178,   32],\n",
       "       [   1,  194, 1153,  194,    2,   78,  228,    5,    6, 1463, 4369,\n",
       "           2,  134,   26,    4,  715,    8,  118, 1634,   14,  394,   20,\n",
       "          13,  119,  954,  189,  102,    5,  207,  110, 3103,   21,   14,\n",
       "          69,  188,    8,   30,   23,    7,    4,  249,  126,   93,    4,\n",
       "         114,    9, 2300, 1523,    5,  647,    4,  116,    9,   35,    2,\n",
       "           4,  229,    9,  340, 1322,    4,  118,    9,    4,  130, 4901,\n",
       "          19,    4, 1002,    5,   89,   29,  952,   46,   37,    4,  455,\n",
       "           9,   45,   43,   38, 1543, 1905,  398,    4, 1649,   26,    2,\n",
       "           5,  163,   11, 3215,    2,    4, 1153,    9,  194,  775,    7,\n",
       "           2,    2,  349, 2637,  148,  605,    2,    2,   15,  123,  125,\n",
       "          68,    2,    2,   15,  349,  165, 4362,   98,    5,    4,  228,\n",
       "           9,   43,    2, 1157,   15,  299,  120,    5,  120,  174,   11,\n",
       "         220,  175,  136,   50,    9, 4373,  228,    2,    5,    2,  656,\n",
       "         245, 2350,    5,    4,    2,  131,  152,  491,   18,    2,   32,\n",
       "           2, 1212,   14,    9,    6,  371,   78,   22,  625,   64, 1382,\n",
       "           9,    8,  168,  145,   23,    4, 1690,   15,   16,    4, 1355,\n",
       "           5,   28,    6,   52,  154,  462,   33,   89,   78,  285,   16,\n",
       "         145,   95,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0],\n",
       "       [   1,   14,   47,    8,   30,   31,    7,    4,  249,  108,    7,\n",
       "           4,    2,   54,   61,  369,   13,   71,  149,   14,   22,  112,\n",
       "           4, 2401,  311,   12,   16, 3711,   33,   75,   43, 1829,  296,\n",
       "           4,   86,  320,   35,  534,   19,  263, 4821, 1301,    4, 1873,\n",
       "          33,   89,   78,   12,   66,   16,    4,  360,    7,    4,   58,\n",
       "         316,  334,   11,    4, 1716,   43,  645,  662,    8,  257,   85,\n",
       "        1200,   42, 1228, 2578,   83,   68, 3912,   15,   36,  165, 1539,\n",
       "         278,   36,   69,    2,  780,    8,  106,   14,    2, 1338,   18,\n",
       "           6,   22,   12,  215,   28,  610,   40,    6,   87,  326,   23,\n",
       "        2300,   21,   23,   22,   12,  272,   40,   57,   31,   11,    4,\n",
       "          22,   47,    6, 2307,   51,    9,  170,   23,  595,  116,  595,\n",
       "        1352,   13,  191,   79,  638,   89,    2,   14,    9,    8,  106,\n",
       "         607,  624,   35,  534,    6,  227,    7,  129,  113,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b836604e-dc82-4f9e-85b1-7775c61ed6bf",
   "metadata": {},
   "source": [
    "데이터를 보니 패딩값(0)이 잘 들어갔고, 데이터의 길이가 서로 일치하는 것을 살펴볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c6f5c9-1ef8-4f00-b98f-75ba3eebeaf1",
   "metadata": {},
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca419e49-ff6d-45f5-8a19-206444238c6a",
   "metadata": {},
   "source": [
    "이제 모델을 구성해보겠습니다. 여기서는 Bidirectional LSTM을 사용하겠습니다.   \n",
    "Bidirectional LSTM은 양방향 LSTM으로 일반적인 LSTM과 달리 역방향으로도 정보를 추출해서 이용할 수 있다는 특징이 있습니다.   \n",
    "일반적으로 양방향 LSTM이 시퀀스 데이터에서 더 많은 정보를 추출할 수 있다는 장점이 있기 때문에 이 모델을 사용하겟습니다.   \n",
    "또한, Embbeding의 input 차원은 vocab_size로 하고, output 차원은 128로 하겠습니다.   \n",
    "Activation Funtion은 시그모이드 함수를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76db3e64-9eef-4e3a-9c51-e31861fc0477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 128)         640000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         98816     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 837,761\n",
      "Trainable params: 837,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "\n",
    "x = layers.Embedding(vocab_size, 128)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5179107-f00f-4016-b4d8-5ec67e1ea8ba",
   "metadata": {},
   "source": [
    "이제 모델을 학습시키겠습니다. 옵티마이져는 adam을 사용하고 critation은 binary_crossentropy를 사용하겠습니다.   \n",
    "배치사이즈는 32로 epochs은 2로 간단한 모델을 학습시켜 봅시다.   \n",
    "또한, 데이터셋에서 가져온 validation data를 파라미터로 줘서 모델의 성능을 해당 데이터로 검증할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "291d1e13-cdb3-44d5-ab37-01371b86fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "704/704 [==============================] - 239s 340ms/step - loss: 0.4252 - accuracy: 0.8072 - val_loss: 0.3142 - val_accuracy: 0.8664\n",
      "Epoch 2/2\n",
      "704/704 [==============================] - 251s 357ms/step - loss: 0.2690 - accuracy: 0.8934 - val_loss: 0.3163 - val_accuracy: 0.8740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f501189b80>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val), validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec07cee-5595-42e8-ba82-f2455e2d7fe0",
   "metadata": {},
   "source": [
    "학습이 잘 마쳤다면 모델을 저장해줍시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "408a33e6-68d8-4128-a01e-ebfbd058afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7c39f-f06b-4bbe-9675-eb7bdbbcb7ee",
   "metadata": {},
   "source": [
    "## 모델 성능 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c9c8e2-c5ad-453b-bf91-28886878aeb9",
   "metadata": {},
   "source": [
    "keras의 load_model을 import하여 모델을 불러오도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0837e84-6859-4252-baea-59dfb80b7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63029c0-4cac-490a-8486-f0c4fa2d2da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b656086-e3ab-4716-a12e-4d40f769a756",
   "metadata": {},
   "source": [
    "이제 모델의 성능을 테스트할 수 있는 함수 `sentiment_predict()`를 만들어 봅시다.   \n",
    "정규표현식을 사용하여 숫자와 알파벳 모두를 제거하고 알파벳을 소문자로 통일시켜 주겠습니다.   \n",
    "또한, `re.sub()`를 사용하여 문자열에서 정규 표현식과 일치하는 부분에 대해 다른 문자열로 대체하겠습니다.  \n",
    "여기서는 두번째 파라미터로 \"\"를 입력하여 숫자와 알파벳 모두를 \"\"로 대체하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c80c46d-d416-4b9d-995c-3abd7fdf0ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = 'I love it.'\n",
    "new_sentence = re.sub(\"[^0-9a-zA-Z ]\", \"\", new_sentence).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6563c3fa-7180-4cc8-92b9-5008843af09f",
   "metadata": {},
   "source": [
    "먼저, 입력으로 받는 new_sentence를 split()하여 단어별로 나눕니다.  \n",
    "그리고, 이를 `word2idx`의 index로 하여 기존에 만들어진 dictionary에서 해당 단어에 해당하는 숫자 값을 가져옵니다.  \n",
    "이 숫자 값들을 모아 `encoded` 리스트를 만들어 모델의 입력값으로 넘겨줄 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "486d061b-0140-407b-9b2f-d6da9ffd127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for word in new_sentence.split():\n",
    "    try:\n",
    "        if word2idx[word] <= vocab_size:\n",
    "            encoded.append(word2idx[word]+3)\n",
    "        else:\n",
    "          # 10,000 이상의 숫자는 <unk> 토큰으로 변환.\n",
    "            encoded.append(2)\n",
    "    except KeyError:\n",
    "        encoded.append(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f474025-ca6d-4efb-85b7-1743d44e7b51",
   "metadata": {},
   "source": [
    "`pad_sequeces()`를 사용하여 인코딩 된 데이터에 패딩을 꼭 넣어주는 것도 잊지 맙시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89e61d7c-b4d4-4002-ba76-a4faf20be0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence = keras.preprocessing.sequence.pad_sequences([encoded], maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66708ac-a33d-4aa1-a707-76f282651d8c",
   "metadata": {},
   "source": [
    "이제, 모델을 통해 성능을 검증해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8c32ead-0760-4a58-b03f-e942e0e917df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.93% 확률로 긍정 리뷰입니다.\n"
     ]
    }
   ],
   "source": [
    "score = float(loaded_model.predict(pad_sequence))\n",
    "if(score > 0.5):\n",
    "    print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "else:\n",
    "    print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984b2f9-f3c7-4736-a10d-cc9bbf215643",
   "metadata": {},
   "source": [
    "학습을 적게 시켰음에도 불구하고,  \n",
    "모델이 꽤 잘 예측한 것을 확인할 수 있습니다.\n",
    "\n",
    "아래는 전체 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "312aae8f-bff9-4d88-bdaa-e96a59fdaa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence, loaded_model):\n",
    "    # 알파벳과 숫자를 제외하고 모두 제거 및 알파벳 소문자화\n",
    "    new_sentence = re.sub(\"[^0-9a-zA-Z ]\", \"\", new_sentence).lower()\n",
    "    encoded = []\n",
    "\n",
    "    for word in new_sentence.split():\n",
    "        try:\n",
    "            if word2idx[word] <= vocab_size:\n",
    "                encoded.append(word2idx[word]+3)\n",
    "            else:\n",
    "              # 10,000 이상의 숫자는 <unk> 토큰으로 변환.\n",
    "                encoded.append(2)\n",
    "        except KeyError:\n",
    "            encoded.append(2)\n",
    "\n",
    "    pad_sequence = keras.preprocessing.sequence.pad_sequences([encoded], maxlen=maxlen)\n",
    "    score = float(loaded_model.predict(pad_sequence))\n",
    "    \n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc084513-8b4a-4cd8-8955-c32aaaed7a22",
   "metadata": {},
   "source": [
    "### 별점 9/10점 리뷰 테스트\n",
    "\n",
    "IMDB 사이트에 접속하여 학습에 사용하지 않은 영화 리뷰들을 테스트해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fac69b34-5f92-4af8-aa99-c41663657a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.31% 확률로 긍정 리뷰입니다.\n"
     ]
    }
   ],
   "source": [
    "test_input = \"Iron Man, one of the most important movies of its kind. When it first came out in 2008, I don't think anybody expected it to be as big a hit as it was. A B-list superhero played by a washed-up actor being produced by a barebones studio? Are you nuts? And then we were all surprised to see how great it was.\"\n",
    "\n",
    "sentiment_predict(test_input, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb66cf5-a7f3-4688-9caa-290184975494",
   "metadata": {},
   "source": [
    "### 별점 1/10점 리뷰 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5458634a-662b-436b-a6e2-0af1270e024b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.99% 확률로 부정 리뷰입니다.\n"
     ]
    }
   ],
   "source": [
    "test_input = \"I had no idea what the film is about but still I seen it because of the overwhelmingly positive reviews although I was put off by the rubbish trailers. After I seen the movie I felt like taking my shoe off and throwing it to the screen, Iron Man has more than enough of what an awful film should have. The movie was even worse than the trailers.\"\n",
    "\n",
    "sentiment_predict(test_input, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c32b96f-371f-4339-9a4f-bb6a0e03d469",
   "metadata": {},
   "source": [
    "모델이 잘 예측하는 것을 확인할 수 있습니다.  \n",
    "이제 모델의 정확도를 높이는 작업은 여러분께 맡기도록 하겠습니다.   \n",
    "감사합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9c387-1b32-49dd-80cd-ccfe40b0959e",
   "metadata": {},
   "source": [
    "# 출처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fa85e-5681-4e70-980a-4ee832e90bb6",
   "metadata": {},
   "source": [
    "[딥러닝을 이용한 자연어 처리 입문](https://wikidocs.net/21703)   \n",
    "[정규표현식](https://ko.wikipedia.org/wiki/%EC%A0%95%EA%B7%9C_%ED%91%9C%ED%98%84%EC%8B%9D)   \n",
    "[Keras Bidirectional LSTM on IMDB](https://keras.io/examples/nlp/bidirectional_lstm_imdb/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
