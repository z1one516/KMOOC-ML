{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예수께서 이르시되 내가 곧 길이요 진리요 생명이니 나로 말미암지 않고는 아버지께로 올 자가 없느니라.(요14:6)\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/MLwithPython.png?raw=true\" width=1000></center>\n",
    "\n",
    "__NOTE:__ The following materials have been compiled and adapted from the numerous sources including my own. Please help me to keep this tutorial up-to-date by reporting any issues or questions. Send any comments or criticisms to `idebtor@gmail.com` Your assistances and comments will be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 7-1 강: 순방향 신경망$^{feed-forward \\ neural \\ network}$\n",
    "\n",
    "## 학습 목표\n",
    "- 순방향  신경망의 신호를 처리한다.\n",
    "\n",
    "## 학습 내용\n",
    "- 순방향 신경망 신호표기 \n",
    "- 순방향 신경망 신호처리\n",
    "- 가중치 표기법\n",
    "- 순방향 신경망 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 다층 신경망의 신호처리\n",
    "\n",
    "세 층으로 이루어진 뉴론들이 다음 층이나 이전 층의 다른 모든 뉴론들과 연결되어 있는 것이 복잡해 보입니다.  신호가 입력층으로 들어오고, 여러 뉴론을 지나서 출력이 되는 것이 복잡해 보이지만, 사실상 따지고 보면 따라가기가 복잡한 곱셈과 덧셈에 불과합니다.  물론, 우리가 컴퓨터를 사용하여 이런 계산을 다 처리하겠지만, 우선적으로 신경망 내부에서 실제로 어떤 일이 일어나는지에 대해 이해하는 것이 먼저 필요합니다. 순방향 신경망의 신호가 어떻게 처리되는지부터 시작해보죠. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 순방향 신경망의 신호처리\n",
    "아래와 같이 3개의 층에 각각 2개, 3개, 2개의 뉴런이 구성된 신경망을 이용해 순방향 신호처리가 어떻게 이루어지 계산해봅시다. 다음은 신경망에 들어가는 두 개의 입력과 각 층의 뉴론들과 가중치의 표기들을 보여줍니다.  $Z$는 각 노드의 입력을, $A$는 각 노드의 출력을 나타냅니다. 괄호(`[]` 혹은 `()`)로 표기된 윗첨자는 층의 일련번호이며, $L$은 보통 전체 층의 수, 각 층은 $l$(소문자 엘)로 표시합니다. 예를 들어, Z$^{[1]}$은 층 1, 즉 은닉층(1)의 입력을 나타내며, A$^{[2]}$은 층 2, 즉 출력층(2)의 출력을 나타냅니다. $X$는 신경망에 입력하는 특성 행렬이며, $A^{[0]}$와 항상 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch7/2-3-2NN-Weights.PNG?raw=true\" width=\"600\">\n",
    "<center>그림 1: 신경망의 입력과 출력 및 가중치 $W_{ij}$ 표기법</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W$는 두 층 사이에 가중치 행렬을 나타내지만 연구자들의 취향에 따라 서로 방법으로 다르게 표기하기도 하여 처음 기계학습을 공부하는 사람들은 혼돈하기 쉽습니다.  그러나 서로 방법들에도 곧 익숙해집니다. \n",
    "\n",
    "$Z$는 뉴론의 입력과 가중치를 곱한 결과를 합산한 값인데, 순입력$^{net \\ input}$ 혹은 가중치합$^{weighted \\ sum}$이라고 부르기도 합니다.  \n",
    "\n",
    "$A$는 뉴론의 출력을 나타냅니다.  뉴론의 입력 $Z$에 활성화 함수 $g(\\cdot)$를 적용한 값입니다.  $Z$와 $A$를 계산하는 방법은 다음과 같습니다. \n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{Z}^{[l]} &=\\mathbf{W}^{[l]T} \\mathbf{A}^{[l-1]} \\\\\n",
    "  \\mathbf{A}^{[l]} &=\\mathbf{g}(\\mathbf{Z}^{[l]})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 최종 출력은 $\\hat{y}$이라고 표기하며 y hat(햇)이라고 읽습니다. 위의 예제와 같은 신경망의 마지막 노드는 출력층(2)에 있으므로 $\\hat{y} = A^{[2]}$가 됩니다.   \n",
    "\n",
    "이렇게 입력층에서 출발하여 은닉층들을 거쳐서 출력층에서 결과를 얻는 것을 순방향 신경망$^{feed-forward \\ neural \\ network}$ 이라고 부릅니다.  순방향 신경망이 어떻게 신호를 처리하는지 예제를 살펴보기전에 한 가지의 유의할 사항은 가중치 행렬 표기법이 다양하여 혼돈할 수 있다는 점입니다.  표기법에 대해 잠깐 알아본 후 순방향 신경망 신호처리를 다루도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 가중치 W 행렬의 두 가지 표기법 \n",
    "\n",
    "각 층사이에 연결된 가중치(시냅스)를 표기하는 행렬로 표기하는 두 가지 방법이 있습니다. 기계학습을 공부하기 시작하는 사람들에게는 혼돈하기 쉽기 때문에 여기서 정리하고자 합니다. \n",
    "\n",
    "### 2.1.1 $W_{ij}$ 표기법\n",
    "\n",
    "그림1과 같은 표기법을 $W_{ij}$ 표기법이라고 부르고 다음과 같은 행렬로 나타낼수 있습니다. 수학에서 사용하는 행렬과 같은 모습입니다. $W$의 윗첨자는 뒤에 나오는 층의 일련번호입니다. \n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{W}^{(1)} &= \n",
    "  \\begin{pmatrix}\n",
    "    w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13}  \\\\\n",
    "    w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\\n",
    "  \\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 같은 경우의 가중치 $W$와 입력층의 특성 자료 $X$와의 곱을 합산한 것을 $Z^{[l]}$라고 할 때 일반적으로 다음과 같은 식으로 표기하며 가중치 행렬을 전치해야 합니다.  입력층과 은닉층이라면 $l = 1$ 이 되며, 따라서 $A^0$인 경우는 $A^0 = X$가 됩니다.  위의 그림에서 빨간색으로 연결된 은닉층 첫번째 노드의 가중치 합$^{weighted \\ sum}$은 식(1) 행렬에서 첫번째 행이며, 그 결과 값은 $z_1 = w_{11} x_1 + w_{21} x_2$ 입니다.  \n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{Z}^{[l]} &= W^{[l]T} A^{[l-1]}   \\\\  \\\\\n",
    "  \\mathbf{Z}^{[1]} &= W^{[1]T} A^{[0]}   \\\\\n",
    "                   &= \n",
    "  \\begin{pmatrix}\n",
    "    w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13}  \\\\   \n",
    "    w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\\n",
    "  \\end{pmatrix}^T\n",
    "  \\begin{pmatrix}     x_1  \\\\     x_2  \\\\   \\end{pmatrix} \\\\ &= \n",
    "  \\begin{pmatrix}  \\tag{1}\n",
    "    w^{(1)}_{11}  & w^{(1)}_{21} \\\\\n",
    "    w^{(1)}_{12} & w^{(1)}_{22}  \\\\\n",
    "    w^{(1)}_{13} & w^{(1)}_{23} \\\\\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix}    x_1  \\\\     x_2  \\\\   \\end{pmatrix} \\\\ &=\n",
    "  \\begin{pmatrix}\n",
    "    w^{(1)}_{11} x_1 + w^{(1)}_{21}x_2 \\\\\n",
    "    w^{(1)}_{12}x_1  + w^{(1)}_{22}x_2  \\\\\n",
    "    w^{(1)}_{13}x_1  + w^{(1)}_{23}x_2 \\\\\n",
    "  \\end{pmatrix} =\n",
    "  \\begin{pmatrix}  z^{(1)}_1  \\\\  z^{(1)}_2  \\\\  z^{(1)}_3 \\end{pmatrix} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉층의 첫번째 노드의 출력 $A^{[1]}$은 다음과 같습니다. 여기서 $g(\\cdot)$은 활성화 함수입니다.\n",
    "\\begin{align}\n",
    "A^{[1]} = g(Z^{[1]})= \n",
    "  \\begin{pmatrix}\n",
    "    a^{[1]}_1  \\\\ \n",
    "    a^{[1]}_2  \\\\\n",
    "    a^{[1]}_3 \n",
    "  \\end{pmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 $W^T_{ij}$ 표기법\n",
    "$W^T_{ij}$ 표기법은 다음 그림에서 확인할 수 있듯이, $W^{[l]}$의 형상은 ($l$층의 노드 수 x $(l-1)$층의 노드 수)가 됩니다. 예를 들면, 입력층과 은닉층 사이의 $W^{[1]}$는 $3\\times2$ 형상의 행렬이 됩니다. 이미 전치가 된 행렬같은 모양입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch7/2-3-2Weights-Wij.PNG?raw=true\" width=\"600\">\n",
    "<center>그림 2: 신경망의 입력과 출력 및 가중치 $W_{ij}$표기법</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림에서 빨간색으로 연결된 은닉층 첫번째 노드의 가중치 합$^{weighted \\ sum}$은 식(2) 행렬에서 첫번째 행이며, 그 결과 값은 $z_1 = w_{11} x_1 + w_{21} x_2$ 입니다.  \n",
    "\n",
    "가중치 행렬이 이미 전치가 되어 있으므로, 은닉층의 입력을 구할 때 가중치 행렬을 전치하지 않습니다. 입력층과 은닉층 사이의 경우라면 $l = 1$ 이 되며, 따라서 $A^0$인 경우는 $A^0 = X$가 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\mathbf{Z}^{[l]} &= W^{[l]} A^{[l-1]}   \\\\ \\\\\n",
    "  \\mathbf{Z}^{[1]} &= W^{[1]} A^{[0]}   \\\\\n",
    "                   &= \n",
    "  \\begin{pmatrix}\n",
    "    w^{(1)}_{11} & w^{(1)}_{21} \\\\  \\tag{2}\n",
    "    w^{(1)}_{12} & w^{(1)}_{22} \\\\\n",
    "    w^{(1)}_{13} & w^{(1)}_{23}\n",
    "  \\end{pmatrix}\n",
    "  \\begin{pmatrix} x_1  \\\\ x_2  \\\\ \\end{pmatrix} \\\\ &=\n",
    "  \\begin{pmatrix} z^{(1)}_1  \\\\ z^{(1)}_2  \\\\ z^{(1)}_3 \\end{pmatrix} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "은닉층의 첫번째 노드의 출력 $A^{[1]}$은 다음과 같습니다. 여기서 $g(\\cdot)$은 활성화 함수입니다.\n",
    "\\begin{align}\n",
    "A^{[1]} = g(Z^{[1]})= \n",
    "  \\begin{pmatrix}\n",
    "    a^{[1]}_1  \\\\ \n",
    "    a^{[1]}_2  \\\\\n",
    "    a^{[1]}_3 \n",
    "  \\end{pmatrix}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 $W_{ij}$와 $W^T_{ij}$ 표기 방식의 장단점\n",
    "\n",
    "$W_{ij}$방식은 $W$행렬의 형상을 좀 더 쉽게 직관적으로 이해할 수 있는 반면에 순전파를 할 때 W행렬을 전치해야 합니다. $W^T_{ij}$방식을 사용할 경우, 순전파 계산을 단순하게 할 수 있다는 장점이 있지만, 일반적으로 수학에서 사용하는 행렬 인덱싱과 다릅니다.  참고로 Coursera와 스탠퍼드 대학의 앤드루 응$^{Andrew \\ Ng}$교수의 강의에서는 $W^T_{ij}$ [표기법](http://taewan.kim/post/nn_notation/)을 사용합니다.  또 다른 한편에 많은 연구자들이 $W_{ij}$ 형식을 사용하기도 합니다. \n",
    "\n",
    "우리는 둘 다 모두 익숙해질 수 있습니다. 여기서는 일단 $W^T_{ij}$ 표기를 따르며, 필요에 따라 다른 방법을 사용하는 것도 익숙해지면 좋겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 순방향 신경망 예제\n",
    "\n",
    "이 예제에서는 $W^T_{ij}$표기법을 사용한 것에 유의하길 바랍니다. \n",
    "\n",
    "### 3.1 입력 자료 준비\n",
    "제일 먼저, 신경망의 입력은 특성 행렬$^{feature \\ matrix}, \\mathbf{X}^{n\\times m}$입니다. 행렬의 한 열$^{column}$은 한 샘플 $x^{(i)}$의 특성들을 의미합니다. 여기서 우리가 $x_1 = 1, \\ x_2 = 0$ 입력 자료가 있다고 가정합시다. 그러면, 한 샘플$(m=1)$뿐이며, 2개의 특성$(n=2)$을 갖고 있으므로, 다음과 같이 표기할 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{X} \\in  \\mathbb{R}^{nxm} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    "   x^{(1)}_1 & x^{(2)}_1  & \\cdots & x^{(m)}_1\\\\\n",
    "   x^{(1)}_2 & x^{(2)}_2  & \\cdots & x^{(m)}_2\\\\\n",
    "   \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "   x^{(1)}_n & x^{(2)}_n  & \\cdots & x^{(m)}_n\n",
    "\\end{pmatrix} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{x}^{(1)} &= \n",
    "  \\begin{pmatrix}     x^{(1)}_1 \\\\ x^{(1)}_2    \\end{pmatrix} \n",
    "  =\n",
    "  \\begin{pmatrix}    1 \\\\ 0    \\end{pmatrix} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2  임의의 가중치 초기화\n",
    "\n",
    "그러면, 가중치는 어떤 값으로 시작해야 할까요? 좋은 질문입니다. 임의의 값으로 시작하는 것도 나쁜 방법이 아니며, 이는 우리가 이전에 다루었던 선형 분류기의 기울기 값을 초기화할 때 사용했던 방법입니다. 각 층의 노드 사이에 연결마다 가중치가 있어야 하므로 모두 12개가 필요합니다. \n",
    "\n",
    "다음과 같이 입력층과 은닉층 사이의 가중치 $W^{(1)}_{ij}$와 은닉층과 출력층 사이의 가중치$W^{(2)}_{ij}$를 임의로 설정합니다.  코딩을 할 때는 -1부터 1사이 값으로 설정합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch7/2-3-2NN.PNG?raw=true\" width=\"400\">\n",
    "<center>그림 3: 신경망의 입력과 출력 및 가중치(2)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 초기 가중치를 모두 0으로 초기화한다면 어떻게 될까요? 초기 가중치가 모두 0이면 제대로된 학습을 할 수 없게 됩니다. 가중치가 같은 영향을 주기 때문에 은닉층에서 모든 노드에 대해 같은 결과값이 계산되어 여러개의 노드를 가지는 것이 의미가 없어질 것입니다. 따라서 가중치를 초기화할 때에는 같은 값을 부여하는 것이 아니라 임의의 값에서부터 시작하는 것이 더 좋은 방법이 될 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 은닉층의 입력 $Z^{(1)}$ 계산\n",
    "\n",
    "자 그러면, 이제 계산을 시작해봅시다!\n",
    "\n",
    "첫 번째 입력층(0층)은 입력 신호를 표시하는 것 외에는 특별히 하는 일이 없습니다. 물론, 입력에 대해 활성화 함수를 적용하지도 않습니다. \n",
    "\n",
    "두 번째 은닉층에서는 약간의 계산이 필요합니다. 은닉층의 각 노드에서 입력층의 출력과 가중치를 곱하여 합산을 하고, 그 합산한 값에 시그모이드 함수 $y= \\frac{1}{1+e^{-x}}$적용하면 은닉층의 출력 $A^{[1]}$이 계산됩니다. \n",
    "\n",
    "예를 들면, 은닉층의 첫 번째 노드$z^{(1)}_1$에 대하여 계산을 하려면, 먼저 두 개의 가중치 $w_{11}$와 $w_{12}$와 두 개의 입력 $x_1$ 과 $x_2$를 각각 곱하여 합산합니다.  은닉층의 입력을 구하는 것($l = 1$인 경우)은 행렬을 사용하면 다음과 같이 간단히 표현할 수 있습니다.  $g(\\cdot)$는 활성화 함수입니다. \n",
    "\n",
    "\\begin{align}\n",
    "  \\mathbf{Z}^{[l]} &= W^{[l]} A^{[l-1]}   \\\\ \\\\\n",
    "  \\mathbf{Z}^{[1]} &= W^{[1]}A^{[0]} =W^{[1]}X   \\\\ &= \n",
    "  \\begin{pmatrix}\n",
    "    w^{(1)}_{11} & w^{(1)}_{21} \\\\\n",
    "    w^{(1)}_{12} & w^{(1)}_{22} \\\\\n",
    "    w^{(1)}_{13} & w^{(1)}_{23}\n",
    "  \\end{pmatrix} \n",
    "  \\begin{pmatrix} x_1  \\\\ x_2  \\end{pmatrix} \\\\\n",
    "  &= \n",
    "  \\begin{pmatrix} 0.1 & 0.4 \\\\  0.2 & 0.2 \\\\ 0.3 & 0.0 \\end{pmatrix}\n",
    "  \\begin{pmatrix} 1.0  \\\\ 0.0  \\end{pmatrix} \\\\  \n",
    "  &=\n",
    "  \\begin{pmatrix} 0.1  \\\\ 0.2  \\\\ 0.3 \\end{pmatrix} \\\\ \n",
    "  \\\\\n",
    "  \\mathbf{A}^{[1]} &= g(\\mathbf{Z}^{[1]})  \\\\\n",
    "      &= sigmoid(\\begin{pmatrix} 0.1  \\\\ 0.2  \\\\ 0.3 \\end{pmatrix} )\n",
    "        = (\\begin{pmatrix} \\frac{1}{1+e^{-0.1}} \\\\ \n",
    "                                  \\frac{1}{1+e^{-0.2}} \\\\\n",
    "                                  \\frac{1}{1+e^{-0.3}} \\end{pmatrix}) \\\\\n",
    "     &= \\begin{pmatrix} 0.525  \\\\ 0.550   \\\\ 0.574 \\end{pmatrix} \n",
    "\\end{align}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 수식을 코딩하며 아래와 같습니다.  참고로, $lambda^{램다}$는 간단히 한 줄로 정의한 함수를 반환합니다. 여기서는 그 함수가 바로 $g(x)$이며, 이 함수는 인자 x를 하나 전달받아 그에 대한 시그모이드 함수 값을 반환합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.2 0.3]\n",
      "[0.52 0.55 0.57]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "g = lambda x : 1 / (1 + np.exp(-x))\n",
    "x = np.array([1, 0])\n",
    "W1 = np.array([[0.1, 0.4], [0.2, 0.2], [0.3, 0.0]])\n",
    "z1 = np.dot(W1, x)      \n",
    "a1 = g(z1)\n",
    "print(\"{}\".format(np.round(z1, 2)))\n",
    "print(\"{}\".format(np.round(a1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 출력층 계산\n",
    "\n",
    "자, 같은 방법으로 마지막 층(즉 $l = 2$)의 출력을 계산해 봅시다. \n",
    "\n",
    "은닉층의 출력 즉 $A^{[1]}$과 가중치 $W^{[2]}$를 곱하고 합산한 값 $Z^{[2]}$를 출력층의 활성화 함수를 적용하면 출력 $A^{[2]}$를 구할 수 있습니다.  이것이 곧 신경망이 예측한 값 $\\hat{y}$입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\mathbf{Z}^{[l]} &= W^{[l]} A^{[l-1]} \\\\ \n",
    " \\mathbf{Z}^{[2]} &= W^{[2]}A^{[1]} \\\\ \n",
    "  &= \n",
    "  \\begin{pmatrix} w^{(2)}_{11} & w^{(2)}_{21} & w^{(2)}_{31}  \\\\\n",
    "                        w^{(2)}_{12} & w^{(2)}_{22} & w^{(2)}_{32}  \\end{pmatrix}  \\tag{3}\n",
    "  \\begin{pmatrix} a^{(1)}_1  \\\\ a^{(1)}_2 \\\\ a^{(1)}_3 \\end{pmatrix} \\\\ &= \n",
    "  \\begin{pmatrix} 0.3 & 0.2  & 0.1  \\\\ 0.2 & 0.1  & 0.1  \\end{pmatrix}\n",
    "  \\begin{pmatrix} 0.525  \\\\ 0.550  \\\\ 0.574\\end{pmatrix} \\\\  &= \n",
    "  \\begin{pmatrix} 0.325 \\\\ 0.217 \\end{pmatrix} \\\\\n",
    "  \\\\\n",
    "  \\mathbf{A}^{[2]} &= g(\\mathbf{Z}^{[2]})  \\\\\n",
    "      &= sigmoid(\\begin{pmatrix} 0.325 \\\\ 0.217 \\end{pmatrix} )\n",
    "        = \\begin{pmatrix} \\frac{1}{1+e^{  -0.325}}   \\\\\n",
    "                                  \\frac{1}{1+e^{  -0.217}}   \\end{pmatrix} \\\\\n",
    "     &= \\begin{pmatrix} 0.581  \\\\  0.554 \\end{pmatrix} \\\\\n",
    "  \\\\\n",
    "  \\hat{y} &= \\begin{pmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\end{pmatrix}\n",
    "               = \\begin{pmatrix} 0.581  \\\\  0.554 \\end{pmatrix} \\\\\n",
    "\\end{align}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "지금까지의 모든 과정을 코딩하면 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1= [0.52497919 0.549834   0.57444252]\n",
      "a2= [0.58051912 0.55414275]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "g = lambda x : 1 / (1 + np.exp(-x))\n",
    "x = np.array([1, 0])\n",
    "W1 = np.array([[0.1, 0.4], [0.2, 0.2], [0.3, 0.0]])\n",
    "z1 = np.dot(W1, x)\n",
    "a1 = g(z1)\n",
    "W2 = np.array([[0.3, 0.2, 0.1], [0.2, 0.1, 0.1]])\n",
    "z2 = np.dot(W2, a1)\n",
    "a2 = g(z2)\n",
    "print('a1=', a1)\n",
    "print('a2=', yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67338284 0.69124778]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "g = lambda x : 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([1.0, 0.0, 2.0])\n",
    "W1 = np.array([[0.1, 0.4, 0.3], [0.2, 0.2, 0.2], [0.3, 0.0, 0.1], [0.2, 0.1, 0.3]])\n",
    "Z1 = np.dot(W1, x)\n",
    "a1 = g(Z1)\n",
    "W2 = np.array([[0.3, 0.5, 0.1, 0.2], [0.4, 0.3, 0.0, 0.5]])\n",
    "z2 = np.dot(W2, a1)\n",
    "a2 = g(z2)\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 인공 신경망의 다양한 표기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 인공 신경망을 처음 공부할 때 반드시 통과해야 할 관문 중에 하나는 인공 신경망의 표기법입니다. 본 강의는 주로 Andrew Ng 교수의 표기법을 사용하고 있습니다. \n",
    "- Andrew Ng 교수의 표기법 한글 번역본이 [여기](http://taewan.kim/post/nn_notation/) 있으며, 강의\n",
    "[Coursera: Neural Networks and Deep Learning by deeplearning.ai, 4Week: Deep Neural Networks - Deep L-layer neural network](https://www.coursera.org/learn/neural-networks-deep-learning/lecture/7dP6E/deep-l-layer-neural-network)를 참고하길 바랍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "- 순방향 신경망 신호 표기\n",
    "- 순방향 신경망 신호 처리 \n",
    "- 가중치 $𝑊_{ij}$  과 $𝑊_{ij}^𝑇$ 방식\n",
    "- 순방향 신경망 예제와 계산\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
