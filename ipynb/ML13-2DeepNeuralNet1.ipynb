{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "하나님은 모든 사람이 구원을 받으며 진리를 아는데에 이르기를 원하시느니라 (딤전2:4)\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/MLwithPython.png?raw=true\" width=1000></center>\n",
    "\n",
    "__NOTE:__ The following materials have been compiled and adapted from the numerous sources including my own. Please help me to keep this tutorial up-to-date by reporting any issues or questions. Send any comments or criticisms to `idebtor@gmail.com` Your assistances and comments will be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 13-2 강: Deep Neural Net\n",
    "\n",
    "## 학습목표 \n",
    "- 기계학습을 위한 오픈 프레임워크는 무엇이 있는지 알아본다.\n",
    "- TensorFlow, Keras, PyTorch가 무엇인지 이해한다.\n",
    "- CNN을 이용한 MNIST 데이터를 3가지 프레임워크로 학습하는 것을 이해한다. \n",
    "\n",
    "## 학습 내용\n",
    "- 기계학습을 위한 오픈 프레임워크\n",
    "- TensorFlow\n",
    "- Keras\n",
    "- PyTorch\n",
    "- MNIST 데이터셋 분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "# Our own private imports\n",
    "import imp\n",
    "import joy\n",
    "imp.reload(joy)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)   # a good practice for reproducibility and debugging\n",
    "\n",
    "# The following code is used for hiding the warnings and \n",
    "# make this notebook clearer.\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Net 구현\n",
    "\n",
    "지금까지 우리는 신경망의 신호처리를 행렬로 표기하면 개발해왔습니다. 이러한 표기법을 바탕으로 어렵지 않게 딥러닝에 필요한 다층 신경망을 구현할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (1.0 - np.exp(-2 * x))/(\n",
    "            1.0 + np.exp(-2 * x))\n",
    "def tanh_d(x):\n",
    "    return (1 + tanh(x)) * (1 - tanh(x))\n",
    "\n",
    "def sigmoid(x): \n",
    "    #x = np.clip(x, -500, 500)  \n",
    "    return 1 / (1 + np.exp((-x)))\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_d(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile code/mnistDeepNet.py\n",
    "#%load code/mnistDeepNet.py\n",
    "# deep neural net\n",
    "# version 0.1\n",
    "# author: idebtor@gmail.com \n",
    "\n",
    "import sys\n",
    "class DeepNeuralNet(object):\n",
    "    \"\"\" implements a deep neural net. \n",
    "        Users may specify any number of layers.\n",
    "        net_arch -- consists of a number of neurons in each layer \n",
    "    \"\"\"\n",
    "    def __init__(self, net_arch, activate = None, eta = 1.0, epochs = 100, random_seed = 1):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.net_arch = net_arch\n",
    "        self.layers = len(net_arch)\n",
    "        self.W = []\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        self.g       = [lambda x: sigmoid(x)   for _ in range(self.layers)]\n",
    "        self.g_prime = [lambda x: sigmoid_d(x) for _ in range(self.layers)]\n",
    "        \n",
    "        if activate is not None:\n",
    "            for i, (g, g_prime) in enumerate(zip(activate[::2], activate[1::2])):\n",
    "                self.g[i+1] = g\n",
    "                self.g_prime[i+1] = g_prime\n",
    "                \n",
    "        for i in range(len(self.g)):\n",
    "            print(type(self.g[i]), id(self.g[i]))\n",
    "        \n",
    "        #print('X.shape={}, y.shape{}'.format(X.shape, y.shape))\n",
    "        # Random initialization with range of weight values (-1,1)\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        # A place holder [None] is used to indicated \"unused place\".\n",
    "        self.W = [[None]]    ## the first W0 is not used.\n",
    "        for layer in range(self.layers - 1):\n",
    "            w = 2 * np.random.rand(self.net_arch[layer+1], \n",
    "                                   self.net_arch[layer]) - 1\n",
    "            print('layer:', layer, 'shape:', w.shape)\n",
    "            self.W.append(w)  \n",
    "        print('Weight:', self.W)\n",
    "            \n",
    "    def forpass(self, A0):     \n",
    "        Z = [[None]]   # Z0 is not used.\n",
    "        A = []       # A0 = X0 is used. \n",
    "        A.append(A0)\n",
    "        for i in range(1, len(self.W)):\n",
    "            z = np.dot(self.W[i], A[i-1])\n",
    "            Z.append(z)\n",
    "            a = self.g[i](z)\n",
    "            A.append(a)\n",
    "        return Z, A\n",
    "    \n",
    "    def backprop(self, Z, A, Y):\n",
    "        # initialize empty lists to save E and dZ\n",
    "        # A place holder None is used to indicated \"unused place\".\n",
    "        E  = [None for x in range(self.layers)]\n",
    "        dZ = [None for x in range(self.layers)]\n",
    "        \n",
    "        # Get error at the output layer or the last layer\n",
    "        ll = self.layers - 1\n",
    "        error = Y - A[ll]\n",
    "        E[ll] = error   \n",
    "        dZ[ll] = error * self.g_prime[ll](Z[ll]) \n",
    "        \n",
    "        # Begin from the back, from the next to last layer\n",
    "        for i in range(self.layers-2, 0, -1):\n",
    "            E[i]  = np.dot(self.W[i+1].T, E[i+1])\n",
    "            dZ[i] = E[i] * self.g_prime[i](Z[i])\n",
    "       \n",
    "        # Adjust the weights, using the backpropagation rules\n",
    "        m = Y.shape[0] # number of samples\n",
    "        for i in range(ll, 0, -1):\n",
    "            self.W[i] += self.eta * np.dot(dZ[i], A[i-1].T) / m\n",
    "        return error\n",
    "         \n",
    "    def fit(self, X, y):\n",
    "        print('fit')\n",
    "        self.cost_ = []        \n",
    "        for epoch in range(self.epochs):          \n",
    "            Z, A = self.forpass(X)        \n",
    "            cost = self.backprop(Z, A, y)   \n",
    "            self.cost_.append(\n",
    "                 np.sqrt(np.sum(cost * cost)))    \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print('predict')\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z, A = self.forpass(A0)     # forpass\n",
    "        return A[-1]                                       \n",
    "   \n",
    "    def evaluate(self, Xtest, ytest):       # fully vectorized calculation\n",
    "        print('evaluate')\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A3 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A3, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, net_arch, activate = None, \n",
    "             eta = 1.0, epochs = 100, random_seed = 1):\n",
    "    self.eta = eta\n",
    "    self.epochs = epochs\n",
    "    self.net_arch = net_arch\n",
    "    self.layers = len(net_arch)\n",
    "    self.W = []\n",
    "\n",
    "    self.g       = [lambda x: sigmoid(x)   for _ in range(self.layers)]\n",
    "    self.g_prime = [lambda x: sigmoid_d(x) for _ in range(self.layers)]\n",
    "\n",
    "    if activate is not None:\n",
    "        for i, (g, g_prime) in enumerate(zip(activate[::2], activate[1::2])):\n",
    "            self.g[i+1] = g\n",
    "            self.g_prime[i+1] = g_prime\n",
    "\n",
    "    np.random.seed(random_seed)\n",
    "    self.W = [[None]]    ## the first W0 is not used.\n",
    "    for layer in range(self.layers - 1):\n",
    "        w = 2 * np.random.rand(self.net_arch[layer+1], \n",
    "                               self.net_arch[layer]) - 1\n",
    "        self.W.append(w)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNet():\n",
    "    \"\"\" implements a deep neural net. \n",
    "        Users may specify any number of layers.\n",
    "        net_arch -- consists of a number of neurons in each layer \n",
    "    \"\"\"\n",
    "    def __init__(self, net_arch, activate = None, \n",
    "                 eta = 1.0, epochs = 100, random_seed = 1):\n",
    "        pass\n",
    "  \n",
    "    def forpass(self, A0):     \n",
    "        pass\n",
    "    \n",
    "    def backprop(self, Z, A, Y):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass   \n",
    "\n",
    "    def predict(self, X):\n",
    "        pass                                     \n",
    "   \n",
    "    def evaluate(self, Xtest, ytest):      \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forpass(self, A0):     \n",
    "    Z = [[None]] # Z0 is not used.\n",
    "    A = []       # A0 = X0 is used. \n",
    "    A.append(A0)\n",
    "    for i in range(1, len(self.W)):\n",
    "        z = np.dot(self.W[i], A[i-1])\n",
    "        Z.append(z)\n",
    "        a = self.g[i](z)\n",
    "        A.append(a)\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, Z, A, Y):\n",
    "    E  = [None for x in range(self.layers)]\n",
    "    dZ = [None for x in range(self.layers)]\n",
    "\n",
    "    ll = self.layers - 1\n",
    "    error = Y - A[ll]\n",
    "    E[ll] = error   \n",
    "    dZ[ll] = error * self.g_prime[ll](Z[ll]) \n",
    "\n",
    "    for i in range(self.layers-2, 0, -1):\n",
    "        E[i]  = np.dot(self.W[i+1].T, E[i+1])\n",
    "        dZ[i] = E[i] * self.g_prime[i](Z[i])\n",
    "\n",
    "    m = Y.shape[0] # number of samples\n",
    "    for i in range(ll, 0, -1):\n",
    "        self.W[i] += self.eta * np.dot(dZ[i], A[i-1].T) / m\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joy\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the input data and labels for XOR\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "print(X, \"\\n\", y)\n",
    "\n",
    "# Initialize the deep neural net with\n",
    "dnn = DeepNeuralNet([2, 4, 2, 1], eta = 0.9, epochs = 10000)  \n",
    "\n",
    "# training the deep neural net objcet with X, y\n",
    "dnn.fit(X, y)             \n",
    "    \n",
    "Ao = dnn.predict(X.T)\n",
    "for x, yhat in zip(X.T, Ao.T):\n",
    "    print(x, np.round(yhat, 3))\n",
    "\n",
    "joy.plot_decision_regions(X.T, y, dnn)   \n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joy\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "dnn = DeepNeuralNet([2, 4, 3, 1], eta = 0.5, epochs = 5000).fit(X, y)   \n",
    "\n",
    "joy.plot_decision_regions(X.T, y, dnn)   \n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 오차(self.cost_)의 시각화 \n",
    "\n",
    "신경망을 학습시키면서 발생하는 오차(손실)를 MnistMiniBatch객체의 속성 `cost_`에 저장되어 있습니다. 이를 시각화해서 신경망이 어떻게 학습을 하였는지, 손실을 최소화하는 방향을 수렴하였는지 분석할 수 있습니다.  다음 셀의 코드를 실행해 봅시다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "dnn = DeepNeuralNet([2, 4, 1], eta = 0.5, epochs = 5000).fit(X, y)   \n",
    "\n",
    "joy.plot_decision_regions(X.T, y, dnn)   \n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(dnn.cost_)), dnn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Squared Sum of Errors')\n",
    "plt.title('DeepNeuralNet:{}'.format(dnn.net_arch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dnn = DeepNeuralNet([2, 4, 1], \n",
    "                    eta = 0.5, epochs = 5000).fit(X, y) \n",
    "plt.plot(range(len(dnn.cost_)), dnn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Squared Sum of Errors')\n",
    "plt.title('DeepNeuralNet:{}'.format(dnn.net_arch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "dnn1 = DeepNeuralNet([2,4,1], eta = 0.5, epochs = 5000).fit(X, y) \n",
    "\n",
    "g = [sigmoid, sigmoid_d, sigmoid, sigmoid_d, sigmoid, sigmoid_d]\n",
    "dnn2 = DeepNeuralNet([2,4,2,1], activate=g, eta = 0.5, epochs = 5000).fit(X, y) \n",
    "plt.plot(range(len(dnn1.cost_)), dnn1.cost_, label='{}'.format(dnn1.net_arch))\n",
    "plt.plot(range(len(dnn2.cost_)), dnn2.cost_, label='{}'.format(dnn2.net_arch))\n",
    "plt.title('DeepNeuralNet for XOR')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Squared Sum of Errors')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 층별로 활성화 함수를 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input data and labels for XOR\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "print(X, \"\\n\", y)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "g = [tanh, tanh_d, sigmoid, sigmoid_d, sigmoid, sigmoid_d]\n",
    "dnn1 = DeepNeuralNet([2, 4, 2, 1], activate = g, eta = 0.5, epochs = 2000).fit(X,y)\n",
    "ax[0].plot(range(1, len(dnn1.cost_) + 1), dnn1.cost_)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_ylim([0.0, 1.1])\n",
    "ax[0].set_title('DeepNeuralNet:{}'.format(dnn1.net_arch))\n",
    "\n",
    "g = [tanh, tanh_d, relu, relu_d, tanh, tanh_d]\n",
    "dnn2 = DeepNeuralNet([2, 4, 2, 1], activate = g, eta=0.5, epochs=2000).fit(X, y)\n",
    "ax[1].plot(range(1, len(dnn2.cost_) + 1), dnn2.cost_)\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Sum-squared-error')\n",
    "ax[1].set_ylim([0.0, 1.1])\n",
    "ax[1].set_title('DeepNeuralNet:{}'.format(dnn2.net_arch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input data and labels for XOR\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "print(X, \"\\n\", y)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "g = [tanh, tanh_d, sigmoid, sigmoid_d, sigmoid, sigmoid_d]\n",
    "dnn1 = DeepNeuralNet([2, 18, 4, 1], activate = g, eta = 0.5, epochs = 2000).fit(X,y)\n",
    "ax[0].plot(range(1, len(dnn1.cost_) + 1), dnn1.cost_)\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_ylim([0.0, 1.1])\n",
    "ax[0].set_title('DeepNeuralNet:{}'.format(dnn1.net_arch))\n",
    "\n",
    "g = [tanh, tanh_d, relu, relu_d, tanh, tanh_d]\n",
    "dnn2 = DeepNeuralNet([2, 18, 4, 1], activate = g, eta=0.5, epochs=2000).fit(X, y)\n",
    "ax[1].plot(range(1, len(dnn2.cost_) + 1), dnn2.cost_)\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Sum-squared-error')\n",
    "ax[1].set_ylim([0.0, 1.1])\n",
    "ax[1].set_title('DeepNeuralNet:{}'.format(dnn2.net_arch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "- 기계학습을 위한 오픈 프레임워크는 무엇이 있는지 알아보기.\n",
    "- TensorFlow, Keras, PyTorch가 무엇인지 이해하기.\n",
    "- CNN을 이용한 MNIST 데이터를 3가지 프레임워크로 학습하는 것을 이해하기.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "Rejoice in the Lord always. I will say it again: Rejoice! (Ph4:4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
