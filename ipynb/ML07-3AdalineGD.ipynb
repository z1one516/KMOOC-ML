{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보혜사 곧 아버지께서 내 이름으로 보내실 성령, 그가 너희에게 모든 것을 가르치고 내가 너희에게 말한 모든 것을 생각나게 하리라 평안을 너희에게 끼치노니 곧 나의 평안을 너희에게 주노라 내가 너희에게 주는 것은 세상이 주는 것과 같지 아니하니라 너희는 마음에 근심하지도 말고 두려워하지도 말라\t(요14:26-27)\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/MLwithPython.png?raw=true\" width=1000></center>\n",
    "\n",
    "__NOTE:__ The following materials have been compiled and adapted from the numerous sources including my own. Please help me to keep this tutorial up-to-date by reporting any issues or questions. Send any comments or criticisms to `idebtor@gmail.com` Your assistances and comments will be appreciated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제 7-3 강: 아달라인과 경사하강법\n",
    "\n",
    "## 학습 목표\n",
    "- 아달라인과 퍼셉트론 알고리즘의 차이점을 학습한다.\n",
    "- 비용함수를 통해  오차가 최소되는 방법을 학습한다.\n",
    "- 경사하강법을 통해 최저점을 찾는 방법을 학습한다.\n",
    "\n",
    "## 학습 내용\n",
    "- 아달라인 알고리즘\n",
    "- 비용 함수\n",
    "- 경사하강법 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 아달라인$^{Adaline}$ 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프랭크 로젠발트의 퍼셉트론 알고리즘이 발표된지 수년 후, 1960년에 버나드 위드로우$^{Bernard \\ Widrow}$와 테드 호프$^{Tedd Hoff}$는 퍼셉트론을 향상시킨 새로 인공신경망  __Adaline(Adaptive Linear Neuron)__이란 알고리즘 논문을 발표하였습니다.  \n",
    "\n",
    "아달라인의 가장 중요한 개념을 비용함수를 정의하고 이를 최소화하는 것입니다. 이러한 개념은 좀 더 나은 분류, 회귀, 로지스틱 회귀, SVM와 같은 기계학습의 알고리즘의 만들어내는 계기가 되었습니다.   아달라인이 퍼셉트론과 가장 다른 점은 활성화 함수입니다.  퍼셉트론은 계단 함수를 사용하는 반면에 아달라인은 선형 함수를 사용합니다. 때문에, 아달라인이 사용하는 선형 함수의 출력값은 연속적인 값($\\in \\mathbb{R}$, 실수)입니다. 다시 말하면, 클레스 레이블과 같이, 1, -1과 같은 값이 아닌 연속적인 출력값과 클래스 레이블과 비교하여 손실을 구하여 가중치를 조정합니다.  반면에 퍼셉트론은 클래스 레이블(실제값) 즉 예를 들면, 1 혹은 -1,과 예측값(1 혹은 -1)을  비교하여 가중치를 조정합니다.  아달라인과 포셉트론의 개념을 서로 비교할 수 있도록 다음과 같이 도식화하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch4/PerceptronAlgorithm.PNG?raw=true\" width=\"500\">\n",
    "<center>그림 1: 퍼셉트론 알고리즘 개념도</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch7/adalineAlgorithm.PNG?raw=true\" width=\"500\">\n",
    "<center>그림 2: 아달라인 알고리즘 개념도</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 비용 함수$^{Cost \\ Function}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼셉트론에서는 활성화 함수가 반환하는 예측값 1 또는 -1로 반환한 값과 클래스 레이블을 비교하여, 서로 값이 일치하지 않을 때만 가중치를 조정하였습니다.  \n",
    "\n",
    "그러나 아달라인에서는 활성화 함수의 반환하는 값(사실상 z와 같음)과 클래스 레이블과의 오차가 최소가 되도록 가중치를 조정합니다.  이를 위하여 최소제곱법$^{Sum \\ of \\ Squared \\ Errors(SSE)}$을 이용한 비용함수$^{Cost \\ Function} J(w)$를 다음과 같이 정의합니다.  이 값을 최소가 되도록 가중치를 조정하는 것이 아달라인 알고리즘의 핵심입니다. \n",
    "\n",
    "\\begin{align}\n",
    " SSE &= \\frac{1}{2} \\sum_{i=1}^{m}(target^{(i)} - output^{(i)})^2 \\\\\n",
    "  J(w)    &= \\frac{1}{2} \\sum_{i=1}^{m}(y^{(i)} - \\hat{y}^{(i)})^2  \\tag{1} \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식(1)에 있는 $\\frac{1}{2}$은 곧 우리가 하게 될 미분을 편하게 하기 위하여 첨가한 것인데, 실제적인 계산에는 영향을 미치지 않습니다. m은 훈련자료의 총 갯수입니다. $y^{(i)}$는 i 번째 훈련자료의 클래스 레이블이며 이 값은 이미 우리에게 알려진 값입니다. $\\hat{y}^{(i)}$는 i번째 훈련자료에 대해 아달라인 알고리즘이 예측한 값입니다.  그러므로 비용함수 식(1)을 다음과 같이 쓸 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "J(w) &= \\frac{1}{2} \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)^2 \\\\\n",
    "      &= \\frac{1}{2} \\sum_{i} \\big(y^{(i)} - \\sum_j (w_j x_j^{(i)})\\big)^2  \\tag{2} \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아달라인 알고리즘에서는 $h(z^{(i)}) = identity(z^{(i)}) = z^{(i)}$이며,  $z^{(i)} = w^Tx^{(i)}$의 값은 클래스 레이블(-1, 0, 1)과 같은 정수가 아니라 하나의 실수$^{a \\ real \\ number}$라는 것에 유의해야 합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "식(2) 비용함수 $J(w)$가 복잡하게 보이지만, 간단한 2차 함수식입니다.  식(2)를 잘 관찰해보면, 함수 $J$는 $w$에 대한 2차 함수인 것을 알 수 있습니다.  우리의 목적은 이 2차 비용함수의 최소값을 찾으려는 것입니다.  자, 그러면 일반적으로 2차 함수의 최소값을 어떻게 찾을 수 있는지 기억하나요?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 경사하강법$^{Gradient \\ descent, \\ GD}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들면,  x에 관한 4차식 $f(x) = x^4 - 3x^3 + 2$가 있다면, 이 함수를 최소로 하는 x값은 어떻게 구할 수 있습니까?  \n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= x^4 - 3x^3 + 2 \\\\\n",
    "f'(x) &= 4x^3 - 9x^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "2차 함수의 최소값은 2차 함수의 기울기가 0인 곳에 위치합니다. <span style=\"color:purple\">이와 같이 4차 함수의 최소값도 4차 함수의 기울기가 0인 곳에 위치합니다.</span> 그러므로, 주어진 식을 미분하여 곧 기울기$^{Gradient}$에 대한 식을 구한 후에 기울기가 0인 곳을 찾는 것입니다.  미분한 식이 0이 될 때, $f'(x) = 0$, x값이 함수 $f(x)$가 최소가 되는 값입니다.  이 예제는 $f'(x) = 0$를 만족하는 x의 값이 두 곳이 있지만, 우리가 찾는 곳은 지역 최소값이 아니라 전역 최소값(Global minimum)의 경우입니다.  참고로 지역 최소값에서도 기울기가 0이 되는데, 이런 지점을 안장점$^{saddle \\ point}$라고 합니다. 그러므로 함수 $f(x)$가 최소값을 갖는 정확한 $x$값은 $x = 2.25$ 입니다.  \n",
    "\n",
    "다음 그림을 참고하길 바랍니다. 이렇게 대수적으로 쉽게 구할 수 있는 경우도 있지만, 그렇지 못할 경우도 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch7/gradientDescentPlot.png?raw=true\" width=\"650\">\n",
    "<center>그림 3: 4차 함수의 최소값과 4차 함수의 미분값</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림을 그리는 코드를 보려면 <span style=\"color:purple\">우선 다음 셀을 실행하여 numpy 와 matplotlib 모듈을 불러온 후에, 그 </span>다음 셀의 첫 줄의 #를 삭제하고, 셀을 실행하여 파일을 불러드린 후, 다시 실행하십시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load code/gradientDescentExample1Plot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제를 경사하강법으로 계산하는 코드를 실행하려면, 다음 셀의 첫 줄의 #를 삭제하여 파일을 불러 드린 후, 다시 실행하십시오. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load code/gradientDescentExample1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그런데 식(2)를 우리가 대수적으로 미분하여 한번에 답을 구할 수 없습니다. 왜냐하면, $w$는 단순한 스칼라가 아니고 $w$는 $m$개 요소의 벡터이기 때문입니다.  \n",
    "\n",
    "그래서, 대수적이 아니라 수치적으로, $J(w)$가 최소가 될 수 있는 가중치를 찾아 가는 방법을 __경사하강법$^{Gradient \\ descent, \\ GD}$__이라고 합니다.   다른 말로, 경사(기울기)하강법은 어떤 곡선에서 최소값을 찾아가는 방법입니다. \n",
    "\n",
    "여러분이 산을 내려간다고 상상해보세요. 산을 내려갈 때에 조금씩 조금씩 내려가는 방향으로 발을 내딛는 과정을 반복해야 합니다. 이와 마찬가지로 경사 하강법에서는 내려가는 경사의 방향으로 단계적으로 내려가면서 최저점을 찾습니다.\n",
    "\n",
    "곡선 위의 어떤 한 지점에서 시작하여 경사의 낮은 방향으로 한 걸음씩 내려가고, 내려간 지점에서 또 다시 경사의 낮은 방향으로 한 걸음씩 내려가기를 반복하여 결국에는 최소지점까지 도달하는 방법입니다. \n",
    "\n",
    "왜 최소값을 찾아가야 하는지 기억하시나요?  공부를 열심히 한다고 해서, 그 목적을 잊어 버리면 안되겠죠? 에러(오차) 혹은 비용을 함수로 나타냈으니, 이것을 최소로 하는 것이 곧 우리가 해결하려고 문제의 해답이기 때문입니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch7/gradientDescent.PNG?raw=true\" width=\"400\">\n",
    "<center>그림 4: 경사(기울기) 하강법</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림 4에서 보듯이 경사하강법은 곡선을 따라 일정 크기 즉 스텝$^{Step}$만큼 접선의 기울기(경사) 방향으로 내려가면서 그 값이 최소값인지 판단합니다.  아달라인에서 각 스텝마다 그 다음 스텝 크기를 결정은 그 지점에서의 접선의 기울기$^{Gradient}$와 학습률$^{\\eta}$로 결정됩니다.  그림4에서 관찰할 수 있듯이 기울기가 양수라면, 스텝의 음수값을 취하고, 기울기가 음수라면, 스텝의 양수값을 취하여야 합니다.  그림 4에서 볼 수 있듯이, 최소값으로부터 멀리 떨어져 있을수록 경사값이 크므로 스텝은 클 수 밖에 없으며, 또한 경사가 양수이면 새로운 가중치는 왼쪽으로 움직이도록 가중치가 좀 더 작아지도록 조정이 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러므로, 경사하강법에서 스텝의 크기 즉 가중치의 조정에 해당하는 $\\Delta w$는 아래와 같이 정의합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  \\Delta w &= -\\eta \\Delta{J(w)} \\\\\n",
    "               &=-\\eta \\frac{\\partial{J(w)}}{\\partial{w_j}} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $\\eta^{에타}$는 학습률입니다.  $\\nabla$은 델$^{Del}$연산자라고 하며 편미분을 나타냅니다.  변수가 한 개가 아니라 여러 개일 경우의 미분을 편미분이라고 합니다.  $\\nabla J(w)$는 $J(w)$의 접선의 기울기, 즉 $J(w)$의 미분입니다. 위의 설명에 따라 초기 가중치 $w$의 값에서 시작하여, 아래의 식에 의해 스텝을 달리하며 $w$를 조정합니다. \n",
    "\\begin{align}\n",
    "  w_{new} &= w_{old} - \\eta \\nabla{CostFunction} \\\\\n",
    "  w_j :&= w_j + \\Delta w_j \\\\\n",
    "        &= w_j - \\eta\\nabla{J(w_j)}   \\tag{3} \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m$개의 훈련자료에 대해 아달라인을 수행하고, 이 식에 의해 가중치를 조정함에 따라 $J(w)$가 어떤 값에 수렴하게 되면 이는 제대로 기계학습이 이루어졌다는 것을 의미합니다.  만약 어떤 값에 수렴하지 못하고 발산하게 되면 학습률을 조정해봄으로써 $J(w)$가 수렴할 수도 있습니다.  따라서 아달라인에서는 $J(w)$가 수렴하도록 하는 학습률을 찾는 것이 중요한데, 1보다 훨씬 작은 값을 적용하면 됩니다. 예를 들면, 0.1 혹은 0.01 단위나 혹은 그 이하도 사용해봄직합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple\">실제 데이터에 아달라인을 수행하게 될 때, 학습률을 조정하는것은 매우 까다로울 수 있습니다. 만약 학습률이 크다면, $J(w)$ 가 수렴하지 않을 수 있습니다. 그림 (5) 를 보세요. 학습률이 크기 때문에, $J(w)$ 가 아래와 같이 진자운동을 하는 것 처럼 최소값으로 수렴하지 않는 경우가 생깁니다. 반대로, 너무 작은 학습률로 학습을 시작한다면 언젠가는 수렴하겠지만 아달라인을 수행하는 시간이 상당히 길어질지도 모릅니다. 이 문제를 해결하기 위한 방법을 다양합니다. 많이 사용하는 방법중에 한가지는 매 스텝마다 학습률을 일정한 비율로 작게 하는 겁니다. 큰 학습률로 아달라인을 시작해서 빠르게 경사를 내려가게끔 유도한 다음에 점점 학습률을 작게 하면서 최소값에 수렴하게끔 하는 방법입니다. 여러분이 분석하는 데이터에 따라 $J(w)$ 가 수렴하도록하는 학습률과 그것을 찾는 방법은 다양하기에, 이에 해당하는 내용은 실제로 데이터를 분석할 때에 다양하게 시도해보도록 합시다. </span>\n",
    "\n",
    "<center><img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ch7/InkedInkedInkedGradientDescent2D_1_LI.jpg?raw=true\" width=\"400\">\n",
    "<center>그림 5: 경사(기울기) 하강법 (큰 학습률)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에 설명한 식(2) 비용함수$J(w)$를 $w$에 대해 편미분을 하면 식(4)를 얻습니다. \n",
    "\\begin{align}\n",
    "J(w) = \\frac{1}{2} \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)^2  \\tag{2} \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{J(w)}}{\\partial{w_j}} = -\\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)x_j^{(i)}  \\tag{4} \n",
    "\\end{align}\n",
    "\n",
    "식(4) 대입하여 식(3)에 대입하면 가중치를 조정하는 식을 다음과 같이 구할 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "  w_j &:= w_j - \\eta\\nabla{J(w_j)} \\\\\n",
    "       &:= w_j + \\eta\\sum_i \\big(y^{(i)} - h(z^{(i)}) \\big) x_j^{(i)} \\tag{5} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $x_j^{(i)}$는 $i$번째 훈련자료의 $j$번째 특성 입력값입니다.  단 여기서, 우리가 주의할 것은 모든 가중치가 동시에 조정되어야 하므로, 아달라인 알고리즘의 가중치 조정은 다음과 같습니다. \n",
    "\n",
    "\\begin{align}\n",
    "  w &:= w + \\Delta{w} \n",
    "\\end{align}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 조정은 각 훈련자료의 각 샘플이 입력됨에 따라 점진적으로 되는 것이 아니라 훈련자료의 모든 샘플을 사용하여 한번에 계산을 합니다.  가중치를 조정할 때마다 모든 샘플을 한 묶음$^{batch}$으로 사용한다는 의미에서 이러한 접근 방법을 __배치 경사하강법$^{Batch \\ gradient \\ descent}$__이라고 부릅니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__비용함수$J(w)$의 편미분 과정__\n",
    "\n",
    "미분 참조 공식: ${f\\big(g(x) \\big)}' = f'\\big(g(x)\\big)g'(x) $\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial{J(w)}}{\\partial{w_j}} &= \\frac{\\partial{}}{\\partial{w_j}} \\frac{1}{2}                                                       \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)^2 \\\\\n",
    "   &= \\frac{1}{2}\\sum_{i} 2 \\big(y^{(i)} - h(z^{(i)})\\big) \n",
    "        \\frac{\\partial{}}{\\partial{w_j}} \\big(y^{(i)} - h(z^{(i)})\\big) \\\\\n",
    "   &= \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big) \n",
    "        \\frac{\\partial{}}{\\partial{w_j}} \\big(y^{(i)} - \\sum_{i}w_j x_j^{(i)}\\big) \\\\\n",
    "   &= \\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big) \\big(- \\sum_{i}x_j^{(i)}\\big) \\\\\n",
    "   &= -\\sum_{i} \\big(y^{(i)} - h(z^{(i)})\\big)x_j^{(i)}  \\tag{4} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "## 참고자료 \n",
    "- [아달라인(Adaline)과 경사하강법(Gradient Descend)](https://blog.naver.com/samsjang/220959562205)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 정리\n",
    "- 아달라인 알고리즘\n",
    "- 아달라인과 퍼셉트론 알고리즘 차이\n",
    "- 비용 함수\n",
    "- 경사하강법\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
